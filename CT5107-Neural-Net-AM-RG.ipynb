{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mnist import MNIST\n",
    "import datetime\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### ---------  PART ONE THE SETUP  -------------  ################\n",
    "##\n",
    "## Other REFERENCES\n",
    "## https://arxiv.org/pdf/1606.04838.pdf\n",
    "## http://www.alivelearn.net/deeplearning/dlnd-your-first-neural-network.html\n",
    "## http://neuralnetworksanddeeplearning.com/chap1.html & chap2.html\n",
    "## https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf\n",
    "## https://arxiv.org/ftp/arxiv/papers/1404/1404.1559.pdf\n",
    "## https://pypi.python.org/pypi/python-mnist\n",
    "## https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "## https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "## http://cs231n.github.io/understanding-cnn/\n",
    "## Andrew Ngs lectures on Coursera\n",
    "## http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html\n",
    "##\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## REFERENCE from Michael Madden's LoadDataset ipynb\n",
    "\n",
    "# Use pandas to read the CSV file as a dataframe\n",
    "df = pd.read_csv(\"moons400.csv\")\n",
    "\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y = df['Class'].values\n",
    "\n",
    "X = df.as_matrix() # convert the remaining columns to a numpy array\n",
    "\n",
    "\n",
    "# np.shape returns all dimensions of the array\n",
    "(nsamples, nattribs) = np.shape(X)\n",
    "print (\"X: nsamples =\", nsamples, \", nattribs =\", nattribs)\n",
    "\n",
    "# using sklearn.model_selection.train_test_split to split up data into train and test sets split 70/30\n",
    "train_X, test_X, train_y, test_y = train_test_split(df, y, test_size=0.30)\n",
    "\n",
    "# The x train and test values are all other columns\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "\n",
    "print(\"train: \", train_X.shape, train_y.shape)\n",
    "print(\"test: \", test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the inputs using normalization \n",
    "inputs = preprocessing.normalize(train_X)\n",
    "inputs = np.array(inputs, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train_y\n",
    "target = np.reshape(np.array(target, dtype=np.float64),(dsamples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise the weights for the network based on the input layers, the number of hidden layers, the number of output layers\n",
    "#reference for above https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "def initialise_input_weights(n_inputs, n_hidden_inputs):\n",
    " hidden_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  weight = np.random.randn(n_inputs)*np.sqrt(1/(n_inputs)**(n_hidden_inputs-1))\n",
    "  hidden_layer_weights.append(weight)\n",
    "   \n",
    " input_weights = np.array([hidden_layer_weights])\n",
    " input_weights = np.reshape(input_weights, (n_inputs, n_hidden_inputs))\n",
    " return input_weights; \n",
    "\n",
    "def initialise_output_weights(n_hidden_inputs,n_outputs):\n",
    " output_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  weight = np.random.randn(n_outputs)*np.sqrt(1/(n_outputs)**(n_hidden_inputs-1))\n",
    "  output_layer_weights.append(weight) \n",
    "   \n",
    " output_weights = np.array(output_layer_weights, dtype=np.float64)\n",
    " #if n_outputs == 1:\n",
    "  #output_weights = np.array([output_layer_weights])\n",
    " #elif n_outputs > 1:\n",
    "  #output_weights = np.array([output_layer_weights])  \n",
    "  #output_weights = np.reshape(output_weights, (n_hidden_inputs,n_outputs))   \n",
    "\n",
    " return output_weights;\n",
    "\n",
    "\n",
    "\n",
    "#initialise the bias for the network based on the number of hidden layers and the output layer bias\n",
    "def initialise_bias(n_hidden_layer):\n",
    " hidden_layer_bias = list()    \n",
    " for i in range(n_hidden_layer):\n",
    "  bias = np.random.random(1)[0]\n",
    "  hidden_layer_bias.append(bias)\n",
    " \n",
    " output_layer_bias = [np.random.random(1)[0]]\n",
    " network_bias = [[hidden_layer_bias],[output_layer_bias]]\n",
    " return network_bias;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION FUNCTIONS\n",
    "\n",
    "def sigmoid(inp, weight, bias):\n",
    "    output = np.dot(inp, weight) +bias\n",
    "    output = 1/(1+np.exp(-output))\n",
    "    return output;\n",
    "\n",
    "def get_error(t, o):\n",
    "    error_o = 0.5*((t - o)**2)\n",
    "    return error_o;\n",
    "\n",
    "# FUNCTIONS FOR BACK PROPAGATION OF OUTPUT\n",
    "\n",
    "def sigmoid_deriv(out):\n",
    "    result = out*(1-out)\n",
    "    return result; \n",
    "\n",
    "\n",
    "# convert the above to a function\n",
    "def calc_adjusted_weights(W, deriv):\n",
    "    W = W - (alpha*deriv)\n",
    "    return W;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up weights and biases \n",
    "(train_samples, train_shape) = np.shape(inputs)\n",
    "print (\"train: samples =\", train_samples, \", attribs =\", train_shape)\n",
    "\n",
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = train_shape\n",
    "\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "\n",
    "Weights1 = np.array(initialise_input_weights(n_inputs, n_hidden_inputs), dtype=np.float64)\n",
    "Weights2 = np.reshape(initialise_output_weights(n_hidden_inputs, n_outputs), (n_hidden_inputs,n_outputs))\n",
    "bias = np.array(initialise_bias(n_hidden_layer), dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### ---------  PART TWO TEST THE SIMPLE DATASET  -------------  ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TRAIN THE MODEL #####\n",
    "#print(\"Weights2:\", Weights2)\n",
    "#print(\"Weights1:\", Weights1)\n",
    "iter=0\n",
    "np.seterr(all='ignore') \n",
    "alpha = 0.5\n",
    "#TRAIN MODEL\n",
    "avg_err  =100\n",
    "print(datetime.datetime.now())\n",
    "threshold=1e-5\n",
    "maxrounds=60000\n",
    "dE=1\n",
    "dH = 0\n",
    "err=[]\n",
    "count=[]\n",
    "error = 99.0\n",
    "avg_err = 99.0\n",
    "#for i in range(maxrounds):\n",
    "if abs(avg_err) > threshold:\n",
    "\n",
    "    # START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    "    hidden_layer = sigmoid(inputs, Weights1, bias[0])\n",
    "        \n",
    "    output = sigmoid(hidden_layer, Weights2, bias[1])\n",
    "\n",
    "    # call error function\n",
    "    error = get_error(target, output)\n",
    "    avg_err = np.sum(error)/nsamples\n",
    "\n",
    "    # START OF BACK PROPAGATION FUNCTION CALLS\n",
    "    # REFERENCE http://python3.codes/neural-network-python-part-1-sigmoid-function-gradient-descent-backpropagation/\n",
    "\n",
    "    dE = (output-target)*sigmoid_deriv(output)\n",
    "\n",
    "    dH = np.dot(dE,Weights2.T)*sigmoid_deriv(hidden_layer)\n",
    "\n",
    "    dW2 = np.dot(hidden_layer.T,dE)\n",
    "    dW1 = dH.T.dot(inputs)\n",
    "\n",
    "    Weights2 = Weights2 - alpha*dW2\n",
    "    Weights1 = Weights1 - alpha*dW1\n",
    "        \n",
    "\n",
    "    iter=iter+1\n",
    "    \n",
    "else:\n",
    "    print (\"\\nFinished after \", iter, \" error =\", error)#\"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "    print(datetime.datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TEST THE MODEL #####\n",
    "\n",
    "inputs = preprocessing.normalize(test_X)\n",
    "target = test_y\n",
    "target = np.array(target)\n",
    "\n",
    "error = 99.0\n",
    "\n",
    "error_res = []\n",
    "result = []\n",
    "\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "\n",
    "        \n",
    "    ### START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "    hidden_layer = sigmoid(inputs[i], Weights1, bias[0])\n",
    "        \n",
    "    output = sigmoid(hidden_layer, Weights2, bias[1])\n",
    "\n",
    "    # call error function\n",
    "    error = get_error(target[i], output)\n",
    "    avg_err = np.sum(error)/nsamples\n",
    "    \n",
    "    #  print (\"output:\", output)\n",
    "    result.append(output)\n",
    "        \n",
    "    error_res.append(error)\n",
    "    #print(\"error:\", error)\n",
    "        \n",
    "    ### END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "    \n",
    "result = np.array(result)  \n",
    "error_res = np.array(error_res)\n",
    "target = np.array(target)\n",
    "print(\"Target:\", target)        \n",
    "print(\"Results: \", result)\n",
    "#print(\"Error:\", error_res)\n",
    "\n",
    "#plt.plot(result, target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### ---------  PART THREE TEST THE DIFFICULT DATASET  -------------  ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD MNIST DATA\n",
    "# reference https://pypi.python.org/pypi/python-mnist\n",
    "# https://github.com/sorki/python-mnist/blob/master/mnist/loader.py\n",
    "\n",
    "mndata = MNIST('./mnist')\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "processed_images = mndata.process_images_to_numpy(images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "target = np.array(labels)\n",
    "# print(target[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILTER MNIST DATA and REMAP 6 to 1\n",
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "\n",
    "\n",
    "filtered_labels = []\n",
    "filtered_images = []\n",
    "\n",
    "\n",
    "for i in range(len(target)):\n",
    "    if target[i]==0 or target[i]==6:\n",
    "        filtered_labels.append(target[i])\n",
    "        filtered_images.append(processed_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_labels)):\n",
    "    if filtered_labels[i]==6:\n",
    "        filtered_labels[i]=1\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "# convert the lists to arrays\n",
    "# TRAIN MNIST DATA\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "filtered_images = np.array(filtered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up weights and biases for MNIST data, get nattribs value\n",
    "\n",
    "(train_samples, train_shape) = np.shape(filtered_images)\n",
    "print (\"train: samples =\", train_samples, \", attribs =\", train_shape)\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = train_shape\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# normalise training data\n",
    "# rescale the inputs using normalization \n",
    "mnist_train = preprocessing.normalize(filtered_images)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "input_weights = np.array(initialise_input_weights(n_inputs, n_hidden_inputs))\n",
    "output_weights = np.array(initialise_output_weights(n_hidden_inputs, n_outputs))\n",
    "bias = np.array(initialise_bias(n_hidden_layer))\n",
    "\n",
    "#validate everything\n",
    "print(\"Length of inputs\",len(filtered_images))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", len(input_weights))\n",
    "print(\"output Weights\", len(output_weights))\n",
    "print(\"Network Bias\", len(network_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html\n",
    "# REFERENCE FOR mini batch logci https://wiseodd.github.io/techblog/2016/06/21/nn-sgd/\n",
    "#TRAIN MNIST DATA\n",
    "iter=0\n",
    "np.seterr(all='ignore') \n",
    "alpha = 0.5\n",
    "#TRAIN MODEL\n",
    "avg_err  =100\n",
    "print(datetime.datetime.now())\n",
    "threshold=1e-5\n",
    "maxrounds=60000\n",
    "dE=1\n",
    "dH = 0\n",
    "err=[]\n",
    "count=[]\n",
    "error = 99.0\n",
    "avg_err = 99.0\n",
    "minibatch_size = 1000\n",
    "if abs(avg_err) > threshold:\n",
    "    for i in range(0, mnist_train.shape[0], minibatch_size):\n",
    "        # Get pair of (X, y) of the current minibatch/chunk\n",
    "        inputs = mnist_train[i:i + minibatch_size]\n",
    "        target = filtered_labels[i:i + minibatch_size]\n",
    "\n",
    "        # START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    "        hidden_layer = sigmoid(inputs, Weights1, bias[0])\n",
    "        \n",
    "        output = sigmoid(hidden_layer, Weights2, bias[1])\n",
    "\n",
    "    # call error function\n",
    "        error = get_error(target, output)\n",
    "        avg_err = np.sum(error)/nsamples\n",
    "\n",
    "        dE = (output-target)*sigmoid_deriv(output)\n",
    "\n",
    "        dH = np.dot(dE,Weights2.T)*sigmoid_deriv(hidden_layer)\n",
    "\n",
    "        dW2 = np.dot(hidden_layer.T,dE)\n",
    "        dW1 = dH.T.dot(inputs)\n",
    "\n",
    "        Weights2 = Weights2 - alpha*dW2\n",
    "        Weights1 = Weights1 - alpha*dW1 \n",
    "\n",
    "    iter=iter+1\n",
    "    \n",
    "else:\n",
    "    print (\"\\nFinished after \", iter, \" error =\", error)#\"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "    print(datetime.datetime.now())\n",
    "#print (\"Weights1 adjusted:\", Weights1)\n",
    "#print (\"Weights2 adjusted:\", Weights2)\n",
    "\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "#plt.scatter(count, err)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SETUP MNIST TEST DATA\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "test_images = mndata.process_images_to_numpy(test_images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "test_target = np.array(test_labels)\n",
    "\n",
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "# TEST MNIST DATA\n",
    "\n",
    "filtered_test_labels = []\n",
    "filtered_test_images = []\n",
    "for i in range(len(target)):\n",
    "    if target[i]==0 or target[i]==6:\n",
    "        filtered_test_labels.append(target[i])\n",
    "        filtered_test_images.append(processed_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_test_labels)):\n",
    "    if filtered_test_labels[i]==6:\n",
    "        filtered_test_labels[i]=1\n",
    "        \n",
    "# convert the lists to arrays\n",
    "# TEST MNIST DATA\n",
    "filtered_test_labels = np.array(filtered_test_labels)\n",
    "\n",
    "filtered_test_images = np.array(filtered_test_images)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TEST MNIST DATA\n",
    "inputs = preprocessing.normalize(test_X)\n",
    "target = test_y\n",
    "target = np.array(target)\n",
    "\n",
    "error = 99.0\n",
    "\n",
    "error_res = []\n",
    "result = []\n",
    "\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "\n",
    "        \n",
    "    ### START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "    hidden_layer = sigmoid(inputs[i], Weights1, bias[0])\n",
    "        \n",
    "    output = sigmoid(hidden_layer, Weights2, bias[1])\n",
    "\n",
    "    # call error function\n",
    "    error = get_error(target[i], output)\n",
    "    avg_err = np.sum(error)/nsamples\n",
    "    \n",
    "    #  print (\"output:\", output)\n",
    "    result.append(output)\n",
    "        \n",
    "    error_res.append(error)\n",
    "    #print(\"error:\", error)\n",
    "        \n",
    "    ### END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "    \n",
    "result = np.array(result)  \n",
    "error_res = np.array(error_res)\n",
    "target = np.array(target)\n",
    "print(\"Target:\", target)        \n",
    "print(\"Results: \", result)\n",
    "#print(\"Error:\", error_res)\n",
    "\n",
    "#plt.plot(result, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### ---------  PART FOUR THE ENHANCEMENT  -------------  ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use relu activation in layer\n",
    "def Leaky_relU(x_input):\n",
    "    x_input = np.maximum(0.000001,x_input)\n",
    "    return x_input\n",
    "\n",
    "# use derivative of relu in back prop layer\n",
    "def deriv_Leaky_relU(y_input):\n",
    "    if y_input <= 0:\n",
    "        y_input = 0.000001\n",
    "    elif y_input >0:\n",
    "        y_input = 1        \n",
    "    return y_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
