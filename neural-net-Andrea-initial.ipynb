{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 3) (280,)\n",
      "(120, 3) (120,)\n",
      "length train X 280\n",
      "length test X 120\n",
      "length train y 280\n",
      "length test y 120\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read the CSV file as a dataframe\n",
    "df = pd.read_csv(\"moons400.csv\")\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y = df['Class'].values\n",
    "# using sklearn.model_selection.train_test_split to split up data into train and test sets split 70/30\n",
    "train_X, test_X, train_y, test_y = train_test_split(df, y, test_size=0.30)\n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "print(\"length train X\", len(train_X))\n",
    "print(\"length test X\", len(test_X))\n",
    "print(\"length train y\", len(train_y))\n",
    "print(\"length test y\", len(test_y))\n",
    "\n",
    "# The x train and test values are all other columns\n",
    "#print(train_X)\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "#print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.687458510039\n"
     ]
    }
   ],
   "source": [
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(train_X, train_y)\n",
    "predictions = lm.predict(test_X)\n",
    "predictions[0:5]\n",
    "\n",
    "## The line / model\n",
    "plt.scatter(test_y, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "print (\"Score:\", model.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X): 280\n",
      "len(X[0]): 2\n",
      "len(X[:,0]): 280\n",
      "X: nsamples = 280 , nattribs = 2\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.as_matrix() # convert the remaining train columns to a numpy array\n",
    "test_X = test_X.as_matrix() # convert the remaining test columns to a numpy array\n",
    "\n",
    "# Some examples of working with the data, to look at rows/columns\n",
    "print (\"len(X):\", len(train_X))            # outer array: one per sample\n",
    "print (\"len(X[0]):\", len(train_X[0]))      # each inner array is the attributes of one sample\n",
    "print (\"len(X[:,0]):\", len(train_X[:,0]))  # select column 0 from array\n",
    "\n",
    "# np.shape returns all dimensions of the array\n",
    "(nsamples, nattribs) = np.shape(train_X)\n",
    "print (\"X: nsamples =\", nsamples, \", nattribs =\", nattribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasubset_x: dsamples = 1 , dattribs = 2\n",
      "datasubset_x: [[ 1.66230734 -0.16230399]]\n"
     ]
    }
   ],
   "source": [
    "#Testing with just one epoch for train X\n",
    "datasubset_x = train_X[0:1]\n",
    "(dsamples, dattribs) = np.shape(datasubset_x)\n",
    "print(\"datasubset_x: dsamples =\", dsamples, \", dattribs =\", dattribs)\n",
    "print(\"datasubset_x:\",datasubset_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasubset_y:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Testing with just one epoch for train y\n",
    "datasubset_y = train_y[0:1]\n",
    "print(\"datasubset_y: \",datasubset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15  0.2 ]\n",
      " [ 0.25  0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "#layer 1 weights\n",
    "#print(np.random.random(1)[0])\n",
    "#W11_1 = -0.1\n",
    "#W12_1 = -0.1\n",
    "#W21_1 = -0.1\n",
    "#W22_1 = -0.1\n",
    "# initially assign the weights randomly \n",
    "#W11_1 = np.random.random(1)[0]\n",
    "#W12_1 = np.random.random(1)[0]\n",
    "#W21_1 = np.random.random(1)[0]\n",
    "#W22_1 = np.random.random(1)[0]\n",
    "\n",
    "W11_1 = 0.15\n",
    "W12_1 = 0.2\n",
    "W21_1 = 0.25\n",
    "W22_1 = 0.3\n",
    "\n",
    "#layer 1 bias\n",
    "bias1 = 0.35\n",
    "#bias2_1 = 0.6\n",
    "#bias1_1 = np.random.random(1)[0]\n",
    "#bias2_1 = np.random.random(1)[0]\n",
    "\n",
    "#layer 2 weights\n",
    "W11_2 = 0.40\n",
    "W12_2 = 0.50\n",
    "#W11_2 =  np.random.random(1)[0]\n",
    "#W12_2 =  np.random.random(1)[0]\n",
    "#layer 2 bias\n",
    "#b_2 = 0.5\n",
    "#bias_2 = np.random.random(1)[0]\n",
    "bias_2 = 0.60\n",
    "\n",
    "weights1 = [[W11_1,W12_1],[W21_1,W22_1]]\n",
    "weights1 = np.array(weights1, dtype=np.float64)\n",
    "print(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x [[ 1.66230734 -0.16230399]]\n",
      "data_x [[ 1.66230734 -0.16230399]]\n",
      "bias 0.35\n",
      "Hidden Layer 1 [[ 0.5993461   0.3175392 ]\n",
      " [ 0.76557683  0.3013088 ]]\n",
      "hidden layer 1 after sigmoid  [[ 0.64550669  0.57872442]\n",
      " [ 0.6825633   0.57476243]]\n"
     ]
    }
   ],
   "source": [
    "print(\"data_x\", datasubset_x)\n",
    "datasubset_x = np.array(datasubset_x,dtype=np.float64)\n",
    "print(\"data_x\", datasubset_x)\n",
    "print(\"bias\", bias1)\n",
    "hidden1 = (weights1 * datasubset_x) + bias1 * 1\n",
    "\n",
    "print(\"Hidden Layer 1\", hidden1 )\n",
    "hidden1 = sigmoid(hidden1)\n",
    "print(\"hidden layer 1 after sigmoid \",hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "# g(z) = 1/ 1 + e^-z\n",
    "def sigmoid(z):\n",
    " g = 1/(1 + np.exp(-z))\n",
    " return g;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net of h1:  0.566885303\n",
      "output activation of h1:  0.638044164892\n",
      "net of h2:  0.7168856379999999\n",
      "output activation of h2:  0.671920845018\n",
      "net_output1 1.19117808847\n",
      "output activation  0.766951698199\n"
     ]
    }
   ],
   "source": [
    "#total net input for h_1:\n",
    "i1 = 1.66230734 \n",
    "i2 = -0.16230399\n",
    "\n",
    "\n",
    "net_h1 = W11_1 * i1 + W12_1 * i2 + bias1 *1\n",
    "out_h1 = sigmoid(net_h1)\n",
    "print(\"net of h1: \",net_h1)\n",
    "print(\"output activation of h1: \",out_h1)\n",
    "\n",
    "\n",
    "net_h2 = W21_1 * i1 + W22_1 * i2 + bias1 *1\n",
    "out_h2 = sigmoid(net_h2)\n",
    "print(\"net of h2: \",net_h2)\n",
    "print(\"output activation of h2: \",out_h2)\n",
    "\n",
    "net_output1 = (( W11_2 * out_h1  )+ (W12_2 * out_h2) + bias_2 * 1) \n",
    "output_activation = sigmoid(net_output1)\n",
    "print(\"net_output1\", net_output1)\n",
    "print(\"output activation \", output_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1_2:  0.566885303263\n",
      "a1_2exp:  0.638044164953\n",
      "a2_2:  0.71688563832\n",
      "a2_2exp:  0.671920845089\n",
      "a1_3:  1.19117808853\n",
      "a1_3exp:  0.766951698209\n"
     ]
    }
   ],
   "source": [
    "# a1_2 = f(w11_1*x1 + w12_1*x2 +b1_1 )\n",
    "#print(datasubset_x[0,0])\n",
    "\n",
    "# forward propagation\n",
    "#print(\"data 0,0\", datasubset_x[0,0])\n",
    "#print(\"data 0,1\", datasubset_x[0,1])\n",
    "\n",
    "a1_2 = ((W11_1 * datasubset_x[0,0]) + (W12_1 * datasubset_x[0,1]) + bias1_1 * 1)\n",
    "print(\"a1_2: \",a1_2)\n",
    "    \n",
    "a1_2 = 1/(1 + np.exp(-a1_2))\n",
    "print(\"a1_2exp: \",a1_2)\n",
    "\n",
    "a2_2 = ((W21_1 * datasubset_x[0,0]) + (W22_1 * datasubset_x[0,1]) + bias1_1 * 1)\n",
    "print(\"a2_2: \",a2_2)\n",
    "a2_2 = 1/(1 + np.exp(-a2_2))\n",
    "print(\"a2_2exp: \",a2_2)\n",
    "\n",
    "\n",
    "a1_3_in = ((W11_2 * a1_2) + (W12_2 * a2_2) + bias_2 * 1 )\n",
    "print(\"a1_3: \",a1_3_in)\n",
    "#hW,b(X) = a1_3\n",
    "a1_3_out = 1/(1 + np.exp(-a1_3_in))\n",
    "print(\"a1_3exp: \",a1_3_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  [1]\n",
      "SSE manual:  [ 0.02715576]\n",
      "SSE matrix:  [ 0.02715576]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Total Error SSE = âˆ‘ 1/2(Y-YP)^2 \n",
    "#E_total = 1/2(target_01 - out_01)^2 \n",
    "print(\"target \", datasubset_y)\n",
    "# sum of squared errors of prediction\n",
    "sse = 1/2 * np.power((datasubset_y - a1_3_out), 2)\n",
    "print(\"SSE manual: \",sse)\n",
    "\n",
    "sse_1 = 1/2 * np.power((datasubset_y - output_activation), 2)\n",
    "print(\"SSE matrix: \",sse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivitive_a1_3:  [-0.2330483]\n",
      "Partial derivitive of total error with respect to the output of a1_3:  [-0.2330483]\n",
      "partial_derivitive_output a1_3:  0.178736790823\n",
      "derivitive_sse:  [ 0.02641832]\n",
      "Partial derivitive of a1_2 with respect to w11_2:  0.638044164953\n",
      "Partial derivitive of total SSE with respect to w11_2:  [-0.02657729]\n"
     ]
    }
   ],
   "source": [
    "#The Backwards back propagation\n",
    "\n",
    "# The goal with backpropagation is to update each of the weights in the network so that \n",
    "# they cause the actual output to be closer the target output, thereby minimizing the error \n",
    "# for each output neuron and the network as a whole.\n",
    "\n",
    "#Consider W11_2. We want to know how much a change in W11_2 affects the total error, \n",
    "#aka the partial derivative of a1_3 with respect to W11_2 or the gradient with respect to W11_2\n",
    "\n",
    "#First, how much does the total error change with respect to the output?\n",
    "#E_total = 1/2(target_01 - out_01)^2 + 1/2(target_02 - out_02)^2 \n",
    "#print(\"SSE: \",sse)\n",
    "\n",
    "# The partial derivitive of the total error with respect to the output a1_3\n",
    "# âˆ‚SSE/âˆ‚a1_3 = 2 * 1/2(target- a1_3)^2-1 * -1 + 0 \n",
    "derivitive_a1_3 = 2 * 1/2 * np.power((datasubset_y - a1_3), 2-1) * -1 + 0\n",
    "print(\"derivitive_a1_3: \",derivitive_a1_3)\n",
    "\n",
    "## results in same output as above\n",
    "pd_sse_a1_3 = -(datasubset_y - a1_3)\n",
    "print(\"Partial derivitive of total error with respect to the output of a1_3: \",pd_sse_a1_3)\n",
    "\n",
    "#The partial derivative of the logistic function is the output multiplied by 1 minus the output:\n",
    "# partial derivitive of output a1_3 with respect to the net a1_3\n",
    "# out_o1 (1 - out_01)\n",
    "# a1_3 (1- a1_3)\n",
    "pd_a1_3 = a1_3 * (1 - a1_3)\n",
    "print(\"partial_derivitive_output a1_3: \",pd_a1_3)\n",
    "\n",
    "\n",
    "# âˆ‚SSE / âˆ‚a1_3 = sse(1 - sse)\n",
    "derivitive_sse = sse * (1 - sse) \n",
    "\n",
    "print(\"derivitive_sse: \",derivitive_sse)\n",
    "\n",
    "# How much does the total net input of a1_3 change with respect to W11_2\n",
    "# âˆ‚a1_3/âˆ‚W11_2 = 1 * a1_2 * W11_2^(1-1) + 0 + 0\n",
    "pd_a1_2 = 1 * a1_2 * np.power(W11_2,(1-1)) + 0 + 0\n",
    "print(\"Partial derivitive of a1_2 with respect to w11_2: \",pd_a1_2)\n",
    "\n",
    "# putting it all together\n",
    "# âˆ‚SSE/âˆ‚W11_2 = âˆ‚SSE / âˆ‚a1_3 * âˆ‚a1_3/âˆ‚net_a1_3 * âˆ‚net_a1_3/âˆ‚W11_2\n",
    "pd_sse_w11_2 = pd_sse_a1_3 * pd_a1_3 * pd_a1_2\n",
    "print(\"Partial derivitive of total SSE with respect to w11_2: \",pd_sse_w11_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MATRIX Multiplication\n",
    "\n",
    "# The partial derivitive of the total error with respect to the output a1_3\n",
    "# âˆ‚SSE/âˆ‚a1_3 = 2 * 1/2(target- a1_3)^2-1 * -1 + 0 \n",
    "derivitive_output_activation = 2 * 1/2 * np.power((datasubset_y - output_activation), 2-1) * -1 + 0\n",
    "print(\"derivitive_a1_3: \",derivitive_a1_3)\n",
    "\n",
    "## results in same output as above\n",
    "pd_sse_output_activation = -(datasubset_y - output_activation)\n",
    "print(\"Partial derivitive of total error with respect to the output of a1_3: \",pd_sse_a1_3)\n",
    "\n",
    "#The partial derivative of the logistic function is the output multiplied by 1 minus the output:\n",
    "# partial derivitive of output a1_3 with respect to the net a1_3\n",
    "# out_o1 (1 - out_01)\n",
    "# a1_3 (1- a1_3)\n",
    "pd_output_activation = output_activation * (1 - output_activation)\n",
    "print(\"partial_derivitive_output a1_3: \",pd_output_activation)\n",
    "\n",
    "\n",
    "# âˆ‚SSE / âˆ‚a1_3 = sse(1 - sse)\n",
    "derivitive_sse_1 = sse_1 * (1 - sse_1) \n",
    "\n",
    "print(\"derivitive_sse_1: \",derivitive_sse_1)\n",
    "\n",
    "# How much does the total net input of a1_3 change with respect to W11_2\n",
    "# âˆ‚a1_3/âˆ‚W11_2 = 1 * a1_2 * W11_2^(1-1) + 0 + 0\n",
    "pd_a1_2 = 1 * a1_2 * np.power(W11_2,(1-1)) + 0 + 0\n",
    "print(\"Partial derivitive of a1_2 with respect to w11_2: \",pd_a1_2)\n",
    "\n",
    "# putting it all together\n",
    "# âˆ‚SSE/âˆ‚W11_2 = âˆ‚SSE / âˆ‚a1_3 * âˆ‚a1_3/âˆ‚net_a1_3 * âˆ‚net_a1_3/âˆ‚W11_2\n",
    "pd_sse_w11_2 = pd_sse_output_activation * pd_output_activation * pd_a1_2\n",
    "print(\"Partial derivitive of total SSE with respect to w11_2: \",pd_sse_w11_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delta rule\n",
    "#delta_o1 = -(target_{o1} - out_{o1}) * out_{o1}(1 - out_{o1})\n",
    "\n",
    "#partial E_total\\partial w_5 = delta_o1*out_h1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
