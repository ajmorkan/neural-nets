{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "import sklearn.datasets\n",
    "from sklearn import preprocessing\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "#from mnist import MNIST\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise the weights for the network based on the input layers, the number of hidden layers, the number of output layers\n",
    "#reference for above https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "def initialise_input_weights(n_inputs, n_hidden_inputs):\n",
    " hidden_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  weight = np.random.randn(n_inputs)*np.sqrt(1/(n_inputs)**(n_hidden_inputs-1))\n",
    "  hidden_layer_weights.append(weight)\n",
    "   \n",
    " input_weights = np.array([hidden_layer_weights], dtype=np.float64)\n",
    " input_weights = np.reshape(input_weights, (n_inputs, n_hidden_inputs))\n",
    " return input_weights; \n",
    "\n",
    "def initialise_output_weights(n_hidden_inputs,n_outputs):\n",
    " output_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  weight = np.random.randn(n_outputs)*np.sqrt(1/(n_outputs)**(n_hidden_inputs-1))\n",
    "  output_layer_weights.append(weight) \n",
    "   \n",
    " output_weights = np.array(output_layer_weights, dtype=np.float64)\n",
    " #if n_outputs == 1:\n",
    "  #output_weights = np.array([output_layer_weights])\n",
    " #elif n_outputs > 1:\n",
    "  #output_weights = np.array([output_layer_weights])  \n",
    "  #output_weights = np.reshape(output_weights, (n_hidden_inputs,n_outputs))   \n",
    "\n",
    " return output_weights;\n",
    "\n",
    "#initialise the bias for the network based on the number of hidden layers and the output layer bias\n",
    "def initialise_bias(n_hidden_layer):\n",
    " hidden_layer_bias = list()    \n",
    " for i in range(n_hidden_layer):\n",
    "  bias = np.random.random(1)[0]\n",
    "  hidden_layer_bias.append(bias)\n",
    " \n",
    " output_layer_bias = [np.random.random(1)[0]]\n",
    " network_bias = np.array([hidden_layer_bias,output_layer_bias],dtype=np.float64)\n",
    " return network_bias;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Weights:  [[ 1.08592178  1.28240275]\n",
      " [-1.738609   -0.04807394]]\n",
      "Output Weights:  [[-0.58770022]\n",
      " [ 0.53955745]]\n",
      "Bias: [[ 0.83428961]\n",
      " [ 0.73703044]]\n"
     ]
    }
   ],
   "source": [
    "# Testing weight intialisation\n",
    "n_hidden_layers = 1\n",
    "n_hidden_inputs = 2\n",
    "n_i_inputs = 2\n",
    "n_outputs = 1\n",
    "alpha = 0.5\n",
    "\n",
    "input_weights = initialise_input_weights(n_i_inputs, n_hidden_inputs)\n",
    "print(\"Input Weights: \",input_weights)\n",
    "weights = initialise_output_weights(n_hidden_inputs, n_outputs)\n",
    "print(\"Output Weights: \",weights)\n",
    "\n",
    "bias = initialise_bias(n_hidden_layers)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "\n",
    "# Sigmoid function\n",
    "# g(z) = 1/ 1 + e^-z\n",
    "def sigmoid(z):\n",
    " g = 1.0 / (1.0 + np.exp(-z))\n",
    " return g;\n",
    "\n",
    "def forward_activation(inputs, input_weights, output_weights, bias):\n",
    "    hidden_output_activation = get_hidden_activation(inputs, input_weights, bias[0])\n",
    "    output_activation = get_output_activation(hidden_output_activation, output_weights, bias[1])\n",
    "    \n",
    "    print(\"hidden_output_activation\", hidden_output_activation)\n",
    "    print(\"output_activation\", output_activation)\n",
    "    return hidden_output_activation, output_activation\n",
    "\n",
    "def get_hidden_activation(inputs, input_weights, bias):\n",
    "    sigmoid_out = 0\n",
    "    #hidden_output_list = list()\n",
    "    for i in range(len(input_weights)):\n",
    "        sigmoid_out += sigmoid((input_weights[i] * inputs[i]) + bias * 1)\n",
    "       \n",
    "    hidden_output_activation = np.array(sigmoid_out, dtype=np.float64)\n",
    "    return hidden_output_activation;\n",
    "\n",
    "def get_output_activation(hidden_output_activation, output_weights, bias):\n",
    "    sigmoid_out = 0\n",
    "    #output_list = list()\n",
    "    for i in range(len(output_weights)):\n",
    "        sigmoid_out += sigmoid((output_weights[i] * hidden_output_activation[i]) + bias * 1)\n",
    "\n",
    "    output_activation = np.array(sigmoid_out, dtype=np.float64)\n",
    "    return output_activation;\n",
    "        \n",
    "def get_total_error(targets, output_activation):\n",
    "    total_error = 0\n",
    "    error = targets - output_activation\n",
    "    total_error = 1/2 * np.power(error, 2)\n",
    "    #for i in range(len(output_activation)):\n",
    "        #error = targets - output_activation[i]\n",
    "        #print(\"Error \",error)\n",
    "        #total_error += 1/2 * np.power(error, 2)\n",
    "    return total_error;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(output_weights))\n",
    "print(len(output_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X): 280\n",
      "len(X[0]): 2\n",
      "len(X[:,0]): 280\n",
      "X: nsamples = 280 , nattribs = 2\n",
      "input_weights [[-0.85199067  1.42878575]\n",
      " [ 0.89024346 -0.3576596 ]]\n",
      "output_weights [[ 0.36682157]\n",
      " [ 0.36925838]]\n",
      "bias  [ 0.86924597] [ 0.80042849]\n"
     ]
    }
   ],
   "source": [
    "# Setup Test data required for training\n",
    "\n",
    "# Use pandas to read the CSV file as a dataframe\n",
    "df = pd.read_csv(\"moons400.csv\")\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y = df['Class'].values\n",
    "# using sklearn.model_selection.train_test_split to split up data into train and test sets split 70/30\n",
    "train_X, test_X, train_y, test_y = train_test_split(df, y, test_size=0.30)\n",
    "\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "\n",
    "train_X = train_X.as_matrix() # convert the remaining train columns to a numpy array\n",
    "test_X = test_X.as_matrix() # convert the remaining test columns to a numpy array\n",
    "\n",
    "#print(train_y)\n",
    "\n",
    "# Some examples of working with the data, to look at rows/columns\n",
    "print (\"len(X):\", len(train_X))            # outer array: one per sample\n",
    "print (\"len(X[0]):\", len(train_X[0]))      # each inner array is the attributes of one sample\n",
    "print (\"len(X[:,0]):\", len(train_X[:,0]))  # select column 0 from array\n",
    "\n",
    "inputs = preprocessing.normalize(train_X) # normalise the input data\n",
    "# np.shape returns all dimensions of the input array\n",
    "(nsamples, nattribs) = np.shape(inputs)\n",
    "print (\"X: nsamples =\", nsamples, \", nattribs =\", nattribs)\n",
    "# the actual labeled target values\n",
    "targets = train_y\n",
    "\n",
    "# initialise neural network structure\n",
    "n_i_inputs = nattribs # number of attributes of inputs\n",
    "n_hidden_inputs = 2 # number of hidden input nodes\n",
    "n_hidden_layers = 1 # number of hidden layers\n",
    "n_outputs = 1 # number of output nodes\n",
    "\n",
    "# initialise weights\n",
    "input_weights = initialise_input_weights(n_i_inputs, n_hidden_inputs)\n",
    "output_weights = initialise_output_weights(n_hidden_inputs, n_outputs)\n",
    "bias = initialise_bias(n_hidden_layers)\n",
    "\n",
    "print(\"input_weights\", input_weights)\n",
    "print(\"output_weights\", output_weights)\n",
    "print(\"bias \",bias[0], bias[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_output_activation [ 1.38494812  1.5167372 ]\n",
      "output_activation [ 1.58309526]\n",
      "Total Error  [ 1.2530953   1.2530953   1.2530953   1.2530953   1.2530953   1.2530953\n",
      "  1.2530953   0.17000004  0.17000004  0.17000004  1.2530953   0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  1.2530953   0.17000004  1.2530953   0.17000004\n",
      "  0.17000004  1.2530953   0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  0.17000004  1.2530953   1.2530953   1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  0.17000004  0.17000004  0.17000004  1.2530953\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953   1.2530953\n",
      "  0.17000004  0.17000004  1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953\n",
      "  1.2530953   0.17000004  0.17000004  0.17000004  0.17000004  0.17000004\n",
      "  0.17000004  0.17000004  0.17000004  1.2530953   1.2530953   1.2530953\n",
      "  0.17000004  1.2530953   0.17000004  1.2530953   1.2530953   1.2530953\n",
      "  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004  1.2530953\n",
      "  1.2530953   1.2530953   0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  0.17000004  0.17000004  1.2530953   0.17000004  0.17000004  1.2530953\n",
      "  1.2530953   1.2530953   0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953   1.2530953\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  1.2530953   1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  1.2530953   1.2530953   1.2530953   1.2530953\n",
      "  1.2530953   0.17000004  0.17000004  0.17000004  1.2530953   1.2530953\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  0.17000004  1.2530953   1.2530953   1.2530953   1.2530953   0.17000004\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004\n",
      "  1.2530953   0.17000004  1.2530953   1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  0.17000004  1.2530953\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004\n",
      "  0.17000004  0.17000004  1.2530953   0.17000004  0.17000004  0.17000004\n",
      "  0.17000004  0.17000004  0.17000004  0.17000004  0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  1.2530953   1.2530953   0.17000004  1.2530953\n",
      "  1.2530953   0.17000004  0.17000004  1.2530953   1.2530953   0.17000004\n",
      "  0.17000004  0.17000004  0.17000004  0.17000004  0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  0.17000004  1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004\n",
      "  1.2530953   1.2530953   0.17000004  1.2530953   0.17000004  1.2530953\n",
      "  0.17000004  0.17000004  0.17000004  1.2530953   1.2530953   1.2530953\n",
      "  1.2530953   0.17000004  1.2530953   1.2530953   0.17000004  0.17000004\n",
      "  1.2530953   1.2530953   1.2530953   1.2530953 ]\n"
     ]
    }
   ],
   "source": [
    "# Test Forward Propagation\n",
    "hidden_output_activation, output_activation = forward_activation(inputs, input_weights, output_weights, bias)\n",
    "total_error = get_total_error(output_activation, targets)\n",
    "\n",
    "#print(\"Net inputs\", hidden_net)\n",
    "#print(\"Hidden output_activation\", hidden_output_activation)  \n",
    "#print(\"Net outputs\", out_net)\n",
    "#print(\"Output activation\", output_activation)  \n",
    "\n",
    "#print(\"targets \", targets) \n",
    "print(\"Total Error \",total_error)\n",
    "#epoch_list = list()\n",
    "#epoch_list.append(total_error)\n",
    "#print(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Propagation\n",
    "\n",
    "# The partial derivitive of the total error with respect to the output a1_3/output_activation \n",
    "# ∂Etotal/∂outo1 = 2 * 1/2(target- output_activation)^2-1 * -1 + 0 \n",
    "# outo1= output_activation, Etotal = sse_1\n",
    "# pd = Partial Derivitive, wrt= with respect to\n",
    "def calc_pd_total_error_wrt_output_activation(target, output_activation):\n",
    "    pd_total_error_list = list()\n",
    "    for i in range(len(target)):\n",
    "        pd_total_error = output_activation - target[i]\n",
    "        #pd_total_error = 2 * 1/2 * np.power((target[i] - output_activation ), 2-1) * -1 + 0\n",
    "        pd_total_error_list.append(pd_total_error)\n",
    "      \n",
    "    pd_total_error_wrt_output_activation = np.array(pd_total_error_list, dtype=np.float64)\n",
    "    return pd_total_error_wrt_output_activation;\n",
    "\n",
    "# The partial derivitive of the total error with respect to the output a1_3/output_activation \n",
    "# ∂outo1/∂neto1 = output_activation(1 - output_activation) \n",
    "\n",
    "def calc_pd_output_activation_wrt_net_input(output_activation):\n",
    "    pd_output_activation_wrt_net_input = output_activation * (1 - output_activation) \n",
    "    print(\"partial_derivitive_output with respect to net input: \",pd_output_activation_wrt_net_input)\n",
    "    pd_output_activation_wrt_net_input = np.array(pd_output_activation_wrt_net_input, dtype=np.float64)\n",
    "    return pd_output_activation_wrt_net_input;\n",
    "\n",
    "\n",
    "#The partial derivitive of net output with respect to weight i:\n",
    "# outputs: out_h1 & out_h2 weights w11_2, w12_2\n",
    "def calc_pd_net_output_wrt_weight(hidden_output_activation, weights):\n",
    "    net_output_list=list()\n",
    "    for i in range(len(hidden_output_activation)):\n",
    "        print(\"hidden_output_activation[i] \", hidden_output_activation[i])\n",
    "        print(\"weights[i]\", weights[i])\n",
    "        pd_net_output =  1 * hidden_output_activation[i] * np.power(weights[i],(1-1)) + 0 + 0\n",
    "        net_output_list.append(pd_net_output)\n",
    "    \n",
    "    pd_net_output_wrt_weight = np.array(net_output_list, dtype=np.float64)\n",
    "    #pd_net_output_wrt_weight = np.array(pd_net_output_wrt_weight, dtype=np.float64)\n",
    "    print(\"Partial derivitive of net output with respect to weight: \",pd_net_output_wrt_weight)\n",
    "    \n",
    "    return pd_net_output_wrt_weight;\n",
    "    \n",
    "\n",
    "# The partial derivitive of Etotal with respect to W5\n",
    "# this is for a single weight W5, same process also has to be done for W6\n",
    "# ∂Etotal/∂W11_2 = ∂Etotal/∂outo1 * ∂outo1/∂neto1 * ∂neto1/∂w11_2 \n",
    "def calc_pd_total_error_wrt_weight(pd_total_error_wrt_output_activation, pd_output_activation_wrt_net_input, pd_net_output_wrt_weight ):\n",
    "    pd_total_error_wrt_weight = pd_total_error_wrt_output_activation * pd_output_activation_wrt_net_input * pd_net_output_wrt_weight\n",
    "    #print(\"Partial derivitive of Total Error with respect to weight: \",pd_total_error_wrt_weight)\n",
    "    return pd_total_error_wrt_weight;\n",
    "\n",
    "# Calculate the adjusted input/output weights\n",
    "# ∂Etotal/∂W11_2 = δ_o1 out_h1\n",
    "# Wi^ = Wi - α * ∂Etotal/∂Wi\n",
    "def adjust_weight(weight, pd_total_error_wrt_weight):\n",
    "    adjusted_weight = weight - alpha * pd_total_error_wrt_weight\n",
    "    return adjusted_weight;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_derivitive_output with respect to net input:  [-0.92309534]\n",
      "hidden_output_activation[i]  1.38494811544\n",
      "weights[i] [ 0.36682157]\n",
      "hidden_output_activation[i]  1.51673719677\n",
      "weights[i] [ 0.36925838]\n",
      "Partial derivitive of net output with respect to weight:  [[ 1.38494812]\n",
      " [ 1.5167372 ]]\n",
      "pd_net_output_wrt_weight [[ 1.38494812]\n",
      " [ 1.5167372 ]]\n",
      "Adjusted Output Weights:  [[ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 0.73954747  0.77745219]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]\n",
      " [ 1.37876705  1.47749871]]\n"
     ]
    }
   ],
   "source": [
    "# Test Back Propagation\n",
    "pd_total_error_wrt_output_activation = calc_pd_total_error_wrt_output_activation(targets, output_activation)\n",
    "#print(\"pd_total_error_wrt_output_activation\",pd_total_error_wrt_output_activation)\n",
    "pd_output_activation_wrt_net_input = calc_pd_output_activation_wrt_net_input(output_activation)\n",
    "    \n",
    "pd_net_output_wrt_weight = calc_pd_net_output_wrt_weight(hidden_output_activation, output_weights)\n",
    "print(\"pd_net_output_wrt_weight\", pd_net_output_wrt_weight) \n",
    "\n",
    "transpose = np.transpose(pd_net_output_wrt_weight)\n",
    "#pd_total_error_wrt_weight = calc_pd_total_error_wrt_weight(pd_total_error_wrt_output_activation, pd_output_activation_wrt_net_input, pd_net_output_wrt_weight) \n",
    "pd_total_error_wrt_weight = calc_pd_total_error_wrt_weight(pd_total_error_wrt_output_activation, pd_output_activation_wrt_net_input, transpose) \n",
    "\n",
    "transpose_weights = np.transpose(output_weights)\n",
    "#adjusted_output_weight = adjust_weight(output_weights, pd_total_error_wrt_weight)\n",
    "adjusted_output_weight = adjust_weight(transpose_weights, pd_total_error_wrt_weight)\n",
    "\n",
    "#print(\"Partial derivitive of total error with respect to the output: \",pd_total_error_wrt_output_activation)\n",
    "print(\"Adjusted Output Weights: \",adjusted_output_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP shape pd_total_error_wrt_output_activation (280, 1)\n",
      "np shape pd_output_activation_wrt_net_input (1,)\n",
      "np shape pd_net_output_wrt_weight (2, 1)\n",
      "pd_net_output_wrt_weight [[ 1.38494812]\n",
      " [ 1.5167372 ]]\n",
      "pd_net_output_wrt_weight[-1] [ 1.5167372]\n"
     ]
    }
   ],
   "source": [
    "print(\"NP shape pd_total_error_wrt_output_activation\",np.shape(pd_total_error_wrt_output_activation))\n",
    "print(\"np shape pd_output_activation_wrt_net_input\", np.shape(pd_output_activation_wrt_net_input))\n",
    "print(\"np shape pd_net_output_wrt_weight\", np.shape(pd_net_output_wrt_weight))\n",
    "\n",
    "#print(\"pd_total_error_wrt_output_activation\", np.shape(np.transpose(pd_total_error_wrt_output_activation)))\n",
    "print(\"pd_net_output_wrt_weight\", pd_net_output_wrt_weight)\n",
    "print(\"pd_net_output_wrt_weight[-1]\", pd_net_output_wrt_weight[-1])\n",
    "pd_total_error_wrt_weight = pd_total_error_wrt_output_activation * pd_output_activation_wrt_net_input * pd_net_output_wrt_weight[-1]\n",
    "#* pd_net_output_wrt_weight\n",
    "#print(\"Partial Derivitive of Total Error with respect to w11_2: \",pd_total_error_wrt_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial_derivitive_output with respect to net input:  [-0.53313317 -0.78375453]\n",
      "hidden_output_activation[i]  1.38494811544\n",
      "weights[i] [-0.85199067  1.42878575]\n",
      "hidden_output_activation[i]  1.51673719677\n",
      "weights[i] [ 0.89024346 -0.3576596 ]\n",
      "Partial derivitive of net output with respect to weight:  [[ 1.38494812  1.38494812]\n",
      " [ 1.5167372   1.5167372 ]]\n"
     ]
    }
   ],
   "source": [
    "# hidden layer weights update\n",
    "\n",
    "pd_total_error_wrt_hidden_activation = calc_pd_total_error_wrt_output_activation(targets, hidden_output_activation)\n",
    "\n",
    "pd_hidden_activation_wrt_net_input = calc_pd_output_activation_wrt_net_input(hidden_output_activation)\n",
    "    \n",
    "pd_hidden_net_wrt_weight = calc_pd_net_output_wrt_weight(hidden_output_activation, input_weights)\n",
    "    \n",
    "#pd_total_error_wrt_input_weight = calc_pd_total_error_wrt_weight(pd_total_error_wrt_hidden_activation, pd_hidden_activation_wrt_net_input, pd_hidden_net_wrt_weight) \n",
    "\n",
    "#adjusted_input_weight = adjust_weight(input_weights, pd_total_error_wrt_input_weight)\n",
    "\n",
    "#print(\"Adjusted Input Weights: \",adjusted_input_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-3adc974dd343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# 3. Update output neuron weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw_ho\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "# initialise neural network structure\n",
    "n_i_inputs = nattribs # number of attributes of inputs\n",
    "n_hidden_inputs = 2 # number of hidden input nodes\n",
    "n_hidden_layers = 1 # number of hidden layers\n",
    "n_outputs = 1 # number of output nodes\n",
    "\n",
    "# initialise weights\n",
    "input_weights = initialise_input_weights(n_i_inputs, n_hidden_inputs)\n",
    "output_weights = initialise_output_weights(n_hidden_inputs, n_outputs)\n",
    "bias = initialise_bias(n_hidden_layers)\n",
    "\n",
    "pd_total_error_wrt_output_activation\n",
    "pd_total_error_wrt_weight\n",
    "\n",
    "# 3. Update output neuron weights\n",
    "for o in range(len(n_outputs)):\n",
    "    for w_ho in range(len(output_weights)):\n",
    "        # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "        pd_error_wrt_weight = pd_total_error_wrt_output_activation[o] * pd_total_error_wrt_weight\n",
    "\n",
    "        # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "        output_weights[w_ho] -= alpha * pd_error_wrt_weight\n",
    "\n",
    "        # 4. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
