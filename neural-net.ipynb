{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mnist import MNIST\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X): 400\n",
      "len(X[0]): 3\n",
      "len(X[:,0]): 400\n",
      "X: nsamples = 400 , nattribs = 3\n",
      "(280, 3) (280,)\n",
      "(120, 3) (120,)\n",
      "length train X 280\n",
      "length test X 120\n",
      "length train y 280\n",
      "length test y 120\n",
      "train:  (280, 2) (280,)\n",
      "test:  (120, 2) (120,)\n"
     ]
    }
   ],
   "source": [
    "# set up training rate alpha\n",
    "alpha = 0.5\n",
    "\n",
    "# Use pandas to read the CSV file as a dataframe\n",
    "df = pd.read_csv(\"moons400.csv\")\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "y = df['Class'].values\n",
    "#print(df)\n",
    "\n",
    "# The x values are all other columns\n",
    "#del df['Class']    # drop the 'Class' column from the dataframe\n",
    "X = df.as_matrix() # convert the remaining columns to a numpy array\n",
    "# Some examples of working with the data, to look at rows/columns\n",
    "print (\"len(X):\", len(X))            # outer array: one per sample\n",
    "print (\"len(X[0]):\", len(X[0]))      # each inner array is the attributes of one sample\n",
    "print (\"len(X[:,0]):\", len(X[:,0]))  # select column 0 from array\n",
    "\n",
    "# np.shape returns all dimensions of the array\n",
    "(nsamples, nattribs) = np.shape(X)\n",
    "print (\"X: nsamples =\", nsamples, \", nattribs =\", nattribs)\n",
    "\n",
    "# using sklearn.model_selection.train_test_split to split up data into train and test sets split 70/30\n",
    "train_X, test_X, train_y, test_y = train_test_split(df, y, test_size=0.30)\n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "print(\"length train X\", len(train_X))\n",
    "print(\"length test X\", len(test_X))\n",
    "print(\"length train y\", len(train_y))\n",
    "print(\"length test y\", len(test_y))\n",
    "\n",
    "# The x train and test values are all other columns\n",
    "#print(train_X)\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "#print(test_X)\n",
    "\n",
    "print(\"train: \", train_X.shape, train_y.shape)\n",
    "print(\"test: \", test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasubset: dsamples = 280 , dattribs = 2\n"
     ]
    }
   ],
   "source": [
    "#datasubset_x = X[0:2]\n",
    "datasubset_x = train_X[0:2]\n",
    "datasubset_x = train_X\n",
    "(dsamples, dattribs) = np.shape(datasubset_x)\n",
    "print(\"datasubset: dsamples =\", dsamples, \", dattribs =\", dattribs)\n",
    "\n",
    "# print(datasubset_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the inputs using normalization \n",
    "inputs = preprocessing.normalize(datasubset_x)\n",
    "# inputs = datasubset_x\n",
    "# inputs[0,0] = 0.05\n",
    "# inputs[0,1] = 0.1\n",
    "# print(inputs)\n",
    "inputs = np.array(inputs, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target = y[0:20]\n",
    "target = train_y[0:2]\n",
    "target = train_y\n",
    "# target = 0.01\n",
    "#print(target)\n",
    "target = np.array(target, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training rate alpha\n",
    "alpha = 0.1\n",
    "\n",
    "# initialise the weights for the network based on the input layers, the number of hidden layers, the number of output layers\n",
    "def initialise_input_weights(n_inputs, n_hidden_inputs):\n",
    " hidden_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  #for j in range(n_inputs):\n",
    "   weight = np.random.randn(n_inputs)*np.sqrt(1/(n_inputs)**(n_hidden_inputs-1)) \n",
    "   #weight = np.random.random(1)[0]\n",
    "    #reference for above https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "   hidden_layer_weights.append(weight)\n",
    " \n",
    " input_weights = np.array([hidden_layer_weights])\n",
    " input_weights = np.reshape(input_weights, (n_inputs, n_hidden_inputs))\n",
    " return input_weights; \n",
    "\n",
    "def initialise_output_weights(n_hidden_inputs,n_outputs):\n",
    " output_layer_weights = list()\n",
    " for i in range(n_outputs):\n",
    "  for j in range(n_hidden_inputs):\n",
    "   weight = np.random.random(1)[0]\n",
    "   output_layer_weights.append(weight) \n",
    "  \n",
    " if n_outputs == 1:\n",
    "  output_weights = np.array([output_layer_weights])\n",
    " elif n_outputs > 1:\n",
    "  output_weights = np.array([output_layer_weights])  \n",
    "  output_weights = np.reshape(output_weights, (n_hidden_inputs,n_outputs))\n",
    " \n",
    " return output_weights;\n",
    "    \n",
    "#initialise the bias for the network based on the number of hidden layers and the output layer bias\n",
    "def initialise_bias(n_hidden_layer):\n",
    " hidden_layer_bias = list()    \n",
    " for i in range(n_hidden_layer):\n",
    "  bias = np.random.random(1)[0]\n",
    "  hidden_layer_bias.append(bias)\n",
    " \n",
    " output_layer_bias = [np.random.random(1)[0]]\n",
    " network_bias = [[hidden_layer_bias],[output_layer_bias]]\n",
    " return network_bias;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION USING FUNCTIONS\n",
    "# do the above using a function\n",
    "# convert calculations to function for hidden layer\n",
    "def get_sigmoid_output(weights, input, bias):\n",
    "    hidden_layer = input * weights \n",
    "    hidden_layer = np.sum(hidden_layer, axis=1) + bias\n",
    "    hidden_layer = 1/(1+np.exp(-hidden_layer))\n",
    "    return hidden_layer;\n",
    "\n",
    "def get_output(sigmoid_out, weights, bias):\n",
    "    output = (sigmoid_out*weights)\n",
    "    output = 1/(1+np.exp(-(np.sum(output)+bias)))\n",
    "    return output;\n",
    "\n",
    "# get the error \n",
    "# error_o = (0.5*(target - output)**2)\n",
    "# print(error_o)\n",
    "\n",
    "#convert to a function\n",
    "# I think this needs to change to accept a numpy array as well\n",
    "# we should end up with a data frame we can plot against the number of executions\n",
    "def get_error(t, o):\n",
    "    error_o = 0.5*((t - o)**2)\n",
    "    return error_o;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR BACK PROPAGATION OF OUTPUT\n",
    "# calculate derivative of error at output layer\n",
    "\n",
    "# deriv_wrt_out = -(target - output)\n",
    "# change above to function\n",
    "def deltaErr_wrt_out(targ, outp):\n",
    "    result = -(targ - outp)\n",
    "    return result;  \n",
    "\n",
    "# calculate the derivation of the error output wrt the net\n",
    "# derivout_wrt_net = output*(1-output)\n",
    "# change above to function\n",
    "def deltaOut_wrt_net(outp):\n",
    "    result = outp*(1-outp)\n",
    "    return result;   \n",
    "\n",
    "# change above to function\n",
    "# calculate derivative of error wrt to output layer weight OLW_Deriv\n",
    "def deltaErr_ow(deriv_wrt_out, derivout_wrt_net, activation):\n",
    "    OLW_Deriv = deriv_wrt_out*derivout_wrt_net*activation\n",
    "   # print(OLW_Deriv)\n",
    "    return OLW_Deriv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FUNCTIONS FOR BACK PROPAGATION OF HIDDEN LAYER\n",
    "# calculate derivative of error at hidden layer\n",
    "\n",
    "# deriv_out_wrt_hL =  Weights2 * deriv_wrt_out *derivout_wrt_net\n",
    "# print (deriv_out_wrt_hL)\n",
    "# convert above to function\n",
    "def deltaOut_hL(deriv_wrt_out, derivout_wrt_net, weights):\n",
    "    deriv_out_wrt_hL = deriv_wrt_out *derivout_wrt_net *  weights\n",
    "    return deriv_out_wrt_hL;\n",
    "\n",
    "# deriv_out_wrt_nethL = activation*(1-activation)\n",
    "# convert above to function\n",
    "def deltaOut_netHL(activation):\n",
    "    activation = activation*(1-activation)\n",
    "    return activation;\n",
    "\n",
    "# deriv_wrt_wi = deriv_out_wrt_hL*deriv_out_wrt_nethL*input\n",
    "# convert above to function\n",
    "def deltaErr_wi(deriv_out_wrt_hL, deriv_out_wrt_nethL, input):\n",
    "    deriv_wrt_wi = deriv_out_wrt_hL*deriv_out_wrt_nethL*input\n",
    "    return deriv_wrt_wi;\n",
    "\n",
    "# convert the above to a function\n",
    "def calc_adjusted_weights(W, deriv):\n",
    "    W = W - (alpha*deriv)\n",
    "    return W;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: samples = 280 , attribs = 2\n",
      "Length of inputs 280\n",
      "input Weights 2\n",
      "[[2.29338398 0.83278095]\n",
      " [0.33871916 0.43070777]]\n",
      "output Weights 1\n",
      "Network Bias 2\n"
     ]
    }
   ],
   "source": [
    "# set up weights and biases \n",
    "(train_samples, train_shape) = np.shape(inputs)\n",
    "print (\"train: samples =\", train_samples, \", attribs =\", train_shape)\n",
    "\n",
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "# first line of moons.csv\n",
    "#input_X =[[2.07106946, 0.41152931]]\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = train_shape\n",
    "\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# normalise training data\n",
    "# rescale the inputs using normalization \n",
    "# mnist_train = preprocessing.normalize(filtered_images)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "Weights1 = np.array(initialise_input_weights(n_inputs, n_hidden_inputs), dtype=np.float64)\n",
    "Weights2 = np.array(initialise_output_weights(n_hidden_inputs, n_outputs), dtype=np.float64)\n",
    "#network_weights = initialise_weights(n_inputs, n_hidden_inputs, n_outputs)\n",
    "bias = np.array(initialise_bias(n_hidden_layer), dtype=np.float64)\n",
    "\n",
    "print(\"Length of inputs\",len(inputs))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", len(Weights1))\n",
    "print(Weights1)\n",
    "print(\"output Weights\", len(Weights2))\n",
    "print(\"Network Bias\", len(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-21 17:15:38.337388\n"
     ]
    }
   ],
   "source": [
    "# use a loop to iterate through the dataset\n",
    "# and present records 1 by 1\n",
    "# for i in np.nditer(inputs, flags=['external_loop'], order='C'):\n",
    "#for i in range(len(inputs)):\n",
    "#    print (i)\n",
    "#    for t in range((target[i])):## this part is not working because it loops through the whole target array each time\n",
    "#        print(t)  ## not what we want\n",
    "\n",
    "np.seterr(all='ignore') \n",
    "alpha = 0.65\n",
    "#TRAIN MODEL\n",
    "print(datetime.datetime.now())\n",
    "threshold=1e-5\n",
    "maxrounds=100000\n",
    "iter=0\n",
    "error = 99.0\n",
    "while abs(error) > threshold:\n",
    "    for i in range(len(inputs[0:150])):\n",
    "  \n",
    "        # START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "        # test get_sigmoid_output \n",
    "        hidden_layer = get_sigmoid_output(Weights1, inputs[i], bias[0])\n",
    "        #print (\"hidden_layer result:\", hidden_layer)\n",
    "\n",
    "        # test get_output\n",
    "        output = get_output(hidden_layer, Weights2, bias[1])\n",
    "        #print (\"output:\", output)\n",
    "\n",
    "        # call error function\n",
    "        error = get_error(target[i], output)\n",
    "       # print(\"error:\", error)\n",
    "\n",
    "        # END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    "        # START OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "        #test function deltaErr_wrt_out\n",
    "        deltaErr_out = deltaErr_wrt_out(target[i], output)\n",
    "        # print(deltaErr_out)\n",
    "\n",
    "        #test function\n",
    "        deltaOut_net = deltaOut_wrt_net(output)\n",
    "        # print(deltaOut_net)\n",
    "\n",
    "        deltaErrtot_ow = deltaErr_ow(deltaErr_out, deltaOut_net, hidden_layer)\n",
    "        # print(deltaErrtot_ow)\n",
    "\n",
    "        # END OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "\n",
    "        # START BACK PROPAGATION OF HIDDEN LAYER\n",
    "        # calculate derivative of error at hidden layer\n",
    "\n",
    "        # test function deltaOut_hL\n",
    "        deltaErrOut_hL = deltaOut_hL(deltaErr_out, deltaOut_net, Weights2)\n",
    "        # print(deltaErrOut_hL)\n",
    "\n",
    "        #test function \n",
    "        deltaErrOut_netHL = deltaOut_netHL(hidden_layer)\n",
    "        # print(deltaErrOut_netHL)\n",
    "\n",
    "       # print(deltaErrtot_ow)\n",
    "        deltaErrH_wi = deltaErr_wi(deltaErrOut_hL, deltaErrOut_netHL, inputs[i])\n",
    "       # print(deltaErrH_wi)\n",
    "\n",
    "        # same calculation as above but with Weights matrix \n",
    "        # calculate adjusted weights using function\n",
    "        Weights2 = calc_adjusted_weights(Weights2, deltaErrtot_ow)\n",
    "        Weights1 = calc_adjusted_weights(Weights1, deltaErrH_wi)\n",
    "        bias[1] = calc_adjusted_weights(bias[1], deltaErr_out)\n",
    "        bias[0] = calc_adjusted_weights(bias[0], 0.5*np.sum(deltaErrOut_hL, dtype=np.float64))\n",
    "\n",
    "        # END OF BACK PROPAGATION OF HIDDEN LAYER\n",
    "\n",
    "    iter=iter+1\n",
    "    \n",
    "    if (iter > maxrounds):\n",
    "        break\n",
    "\n",
    "print (\"\\nFinished after \", iter, \" error =\", error, \"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "print(datetime.datetime.now())\n",
    "#print (\"Weights1 adjusted:\", Weights1)\n",
    "#print (\"Weights2 adjusted:\", Weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference https://pypi.python.org/pypi/python-mnist\n",
    "# https://github.com/sorki/python-mnist/blob/master/mnist/loader.py\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "mndata = MNIST('./mnist')\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "processed_images = mndata.process_images_to_numpy(images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "target = np.array(labels)\n",
    "# print(target[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "filtered_labels = []\n",
    "filtered_images = []\n",
    "for i in range(len(target)):\n",
    "    if target[i]==0 or target[i]==6:\n",
    "        filtered_labels.append(target[i])\n",
    "        filtered_images.append(processed_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_labels)):\n",
    "    if filtered_labels[i]==6:\n",
    "        filtered_labels[i]=1\n",
    "\n",
    "# print(filtered_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the lists to arrays\n",
    "# TRAIN MNIST DATA\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "filtered_images = np.array(filtered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST TEST DATA\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "test_images = mndata.process_images_to_numpy(test_images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "test_target = np.array(test_labels)\n",
    "\n",
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "# TEST MNIST DATA\n",
    "\n",
    "filtered_test_labels = []\n",
    "filtered_test_images = []\n",
    "for i in range(len(target)):\n",
    "    if target[i]==0 or target[i]==6:\n",
    "        filtered_test_labels.append(target[i])\n",
    "        filtered_test_images.append(processed_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_test_labels)):\n",
    "    if filtered_test_labels[i]==6:\n",
    "        filtered_test_labels[i]=1\n",
    "        \n",
    "# convert the lists to arrays\n",
    "# TEST MNIST DATA\n",
    "filtered_test_labels = np.array(filtered_test_labels)\n",
    "\n",
    "filtered_test_images = np.array(filtered_test_images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_test_labels[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: samples = 11841 , attribs = 784\n",
      "Length of inputs 11841\n",
      "input Weights 784\n",
      "output Weights 1\n",
      "Network Bias 2\n"
     ]
    }
   ],
   "source": [
    "# set up weights and biases for MNIST data, get nattribs value\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "(train_samples, train_shape) = np.shape(filtered_images)\n",
    "print (\"train: samples =\", train_samples, \", attribs =\", train_shape)\n",
    "\n",
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "# first line of moons.csv\n",
    "#input_X =[[2.07106946, 0.41152931]]\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = train_shape\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# normalise training data\n",
    "# rescale the inputs using normalization \n",
    "mnist_train = preprocessing.normalize(filtered_images)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "input_weights = np.array(initialise_input_weights(n_inputs, n_hidden_inputs))\n",
    "output_weights = np.array(initialise_output_weights(n_hidden_inputs, n_outputs))\n",
    "#network_weights = initialise_weights(n_inputs, n_hidden_inputs, n_outputs)\n",
    "network_bias = np.array(initialise_bias(n_hidden_layer))\n",
    "\n",
    "print(\"Length of inputs\",len(filtered_images))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", len(input_weights))\n",
    "print(\"output Weights\", len(output_weights))\n",
    "print(\"Network Bias\", len(network_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "## TEST MODEL\n",
    "inputs = preprocessing.normalize(test_X)\n",
    "target = test_y\n",
    "target = np.array(target)\n",
    "print(len(inputs))\n",
    "print(len(target))\n",
    "#tol=1e-7\n",
    "#maxrounds=1000\n",
    "#iter=0\n",
    "error = 99.0\n",
    "#while abs(error) > tol:\n",
    "error_res = []\n",
    "result = []\n",
    "#result = np.array(result)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "   # print (\"input index: \", inputs[i])\n",
    "#print (\"target index: \", target[i])\n",
    "        \n",
    "    ### START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "    # test get_sigmoid_output \n",
    "    hidden_layer = get_sigmoid_output(Weights1, inputs[i], bias[0])\n",
    "    ##print (\"hidden_layer result:\", hidden_layer)\n",
    "        \n",
    "    # test get_output\n",
    "    output = get_output(hidden_layer, Weights2, bias[1])\n",
    "    #  print (\"output:\", output)\n",
    "    result.append(output)\n",
    "        \n",
    "    # call error function\n",
    "    error = get_error(target[i], output)\n",
    "    error_res.append(error)\n",
    "    #print(\"error:\", error)\n",
    "        \n",
    "    # END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "result = np.array(result)  \n",
    "error_res = np.array(error_res)\n",
    "target = np.array(target)\n",
    "#print(\"Target:\", target)        \n",
    "print(\"Results: \", result)\n",
    "#print(\"Error:\", error_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the NN\n",
    "print(len(target))\n",
    "print(len(result))\n",
    "print(threshold)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i in range(len(target)):\n",
    "        \n",
    "        if (target[i] == 1 and target[i]-result[i] <= threshold):\n",
    "            TP = TP + 1\n",
    "          #  print(\"True Positive: \", target[i], \" - \", result[i])\n",
    "\n",
    "        if (target[i] == 0 and target[i]-result[i] <= threshold):\n",
    "            TN = TN + 1\n",
    "          #  print(\"True Negative: \",target[i], \" - \", result[i])\n",
    "            \n",
    "        if (target[i] == 1 and target[i]-result[i] > threshold):\n",
    "            FP = FP + 1\n",
    "         #   print(\"False Positive: \",target[i], \" - \", result[i])\n",
    "            \n",
    "        if (target[i] == 0 and target[i]-result[i] > threshold):\n",
    "            FN = FN + 1\n",
    "          #  print(\"False Negative: \",target[i], \" - \", result[i])\n",
    "        \n",
    "#print(\"True Positive: \", TP)\n",
    "#print(\"True Negative: \", TN)\n",
    "#print(\"False Positive: \", FP)\n",
    "#print(\"False Negative: \", FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use relu activation in layer\n",
    "def Leaky_relU(x_input):\n",
    "    x_input = np.maximum(0.000001,x_input)\n",
    "    return x_input\n",
    "\n",
    "# use derivative of relu in back prop layer\n",
    "def deriv_Leaky_relU(y_input):\n",
    "    if y_input <= 0:\n",
    "        y_input = 0.000001\n",
    "    elif y_input >0:\n",
    "        y_input = 1        \n",
    "    return y_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
