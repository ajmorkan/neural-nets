{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from sklearn import preprocessing\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up training rate alpha\n",
    "alpha = 0.5\n",
    "\n",
    "# initialise the weights for the network based on the input layers, the number of hidden layers, the number of output layers\n",
    "def initialise_input_weights(n_inputs, n_hidden_inputs):\n",
    " hidden_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  for j in range(n_inputs):\n",
    "   weight = np.random.random(1)[0]\n",
    "   hidden_layer_weights.append(weight)\n",
    " \n",
    " input_weights = np.array([hidden_layer_weights])\n",
    " input_weights = np.reshape(input_weights, (n_inputs, n_hidden_inputs))\n",
    " return input_weights; \n",
    "\n",
    "def initialise_output_weights(n_hidden_inputs,n_outputs):\n",
    " output_layer_weights = list()\n",
    " for i in range(n_outputs):\n",
    "  for j in range(n_hidden_inputs):\n",
    "   weight = np.random.random(1)[0]\n",
    "   output_layer_weights.append(weight) \n",
    "  \n",
    " if n_outputs == 1:\n",
    "  output_weights = np.array([output_layer_weights])\n",
    " elif n_outputs > 1:\n",
    "  output_weights = np.array([output_layer_weights])  \n",
    "  output_weights = np.reshape(output_weights, (n_hidden_inputs,n_outputs))\n",
    " \n",
    " return output_weights;\n",
    "    \n",
    "#initialise the bias for the network based on the number of hidden layers and the output layer bias\n",
    "def initialise_bias(n_hidden_layer):\n",
    " hidden_layer_bias = list()    \n",
    " for i in range(n_hidden_layer):\n",
    "  bias = np.random.random(1)[0]\n",
    "  hidden_layer_bias.append(bias)\n",
    " \n",
    " output_layer_bias = [np.random.random(1)[0]]\n",
    " network_bias = [[hidden_layer_bias],[output_layer_bias]]\n",
    " return network_bias;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "# g(z) = 1/ 1 + e^-z\n",
    "def sigmoid(z):\n",
    " g = 1/(1 + np.exp(-z))\n",
    " return g;\n",
    "\n",
    "# --------------- FORWARD PROPAGATION  --------------------------\n",
    "# return the sigmoid_input_act - sigmoid function sum of the input layer activation \n",
    "# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\n",
    "def get_input_layer_activation(input, input_weights, input_bias):\n",
    " input_layer_activation = input * input_weights\n",
    " #print(\"input_layer_activation: \",input_layer_activation)\n",
    " hidden_layer_activation = np.sum(input_layer_activation, axis = 1) + input_bias * 1\n",
    " #print(\"Sum of Hidden Layer activation: \", hidden_layer_activation)\n",
    " sigmoid_input_activation = sigmoid(hidden_layer_activation)\n",
    " #print(\"Sigmoid function of Sum of Hidden Layer activation: \", sigmoid_input_activation)\n",
    " return sigmoid_input_activation;\n",
    "\n",
    "# get output layer activation for hidden layer\n",
    "# a1_3\n",
    "def get_activation_output(sigmoid_input_activation, output_weights, output_bias):\n",
    " output = (sigmoid_input_activation * output_weights)\n",
    " activation_output = sigmoid(np.sum(output) + output_bias * 1 )\n",
    " #print(\"Activation Output: \",activation_output)   \n",
    " return activation_output;\n",
    " \n",
    "def forward_propagation(input, input_weights, output_weights, input_bias, output_bias):\n",
    " sigmoid_input_activation = get_input_layer_activation(input, input_weights, input_bias)\n",
    " activation_output = get_activation_output(sigmoid_input_activation, output_weights, output_bias)\n",
    " #print(\"Forward Propagation Activation Output: \",activation_output)    \n",
    " return activation_output;   \n",
    "\n",
    "# Calculate the Total Error Sum of Squared Errors = âˆ‘ 1/2(Y-YP)^2 \n",
    "#E_total = 1/2(target_01 - out_01)^2 \n",
    "# sum of squared errors of prediction\n",
    "def calc_error(actual_y, target):\n",
    " error = 1/2 * np.power((actual_y - target), 2)\n",
    " #print(\"Error Total: \",error)\n",
    " return error;\n",
    "\n",
    "# If there is more than 1 node in the output layer, sum up the calc error\n",
    "def calc_total_error(actual_y, target):\n",
    " total_error = 0\n",
    " total_error += calc_error(actual_y, target)\n",
    " return total_error;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR BACK PROPAGATION OF OUTPUT\n",
    "# calculate derivative of error at output layer\n",
    "\n",
    "# Derivitive of error with reference to output\n",
    "# deriv_wrt_out = -(target - output)\n",
    "# change above to function\n",
    "def deltaErr_wrt_out(target, output):\n",
    " result = -(target - output)\n",
    " print(\"Derivitive of error with reference to output:\", result)\n",
    " return result;  \n",
    "\n",
    "# calculate the derivation of the error output wrt the net\n",
    "# derivout_wrt_net = output*(1-output)\n",
    "# change above to function\n",
    "def deltaOut_wrt_net(output):\n",
    " result = output * (1 - output)\n",
    " print(\"Derivitive of error output with reference to net output:\", result)\n",
    " return result;   \n",
    "\n",
    "# calculate derivative of error wrt to output layer weight OLW_Deriv\n",
    "# Output Layer Weight Derivitive\n",
    "def deltaErr_ow(deriv_wrt_out, derivout_wrt_net, activation):\n",
    " OLW_Deriv = deriv_wrt_out * derivout_wrt_net * activation\n",
    " print(\"Derivative of error with reference to output layer weight:\", OLW_Deriv)\n",
    " return OLW_Deriv;\n",
    "    \n",
    "# FUNCTIONS FOR BACK PROPAGATION OF HIDDEN LAYER\n",
    "# calculate derivative of error at hidden layer\n",
    "# deriv_out_wrt_hL =  Weights2 * deriv_wrt_out *derivout_wrt_net\n",
    "# print (deriv_out_wrt_hL)\n",
    "# convert above to function\n",
    "def deltaOut_hL(deriv_wrt_out, derivout_wrt_net, output_weights):\n",
    " deriv_out_wrt_hL = deriv_wrt_out * derivout_wrt_net * output_weights\n",
    " return deriv_out_wrt_hL;\n",
    "\n",
    "# derivitive output in relation to net of hidden layer activation\n",
    "# deriv_out_wrt_nethL = activation*(1-activation)\n",
    "# convert above to function\n",
    "def deltaOut_netHL(activation):\n",
    " activation = activation * (1 - activation)\n",
    " return activation;\n",
    "\n",
    "# Derivitive in relation to input_weights\n",
    "# deriv_wrt_wi = deriv_out_wrt_hL*deriv_out_wrt_nethL*Weights1\n",
    "# convert above to function\n",
    "def deltaErr_wi(deriv_out_wrt_hL, deriv_out_wrt_nethL, input_weights):\n",
    " deriv_wrt_wi = deriv_out_wrt_hL * deriv_out_wrt_nethL * input_weights\n",
    " return deriv_wrt_wi;\n",
    "\n",
    "# convert the above to a function\n",
    "def calc_adjusted_weights(W, deriv):\n",
    " W = W - (alpha * deriv)\n",
    " return W;\n",
    "\n",
    "# delta error with reference to output\n",
    "def delta_error_wrt_output(output, hidden_layer, target):\n",
    " deltaErr_out = deltaErr_wrt_out(target, output)\n",
    " deltaOut_net = deltaOut_wrt_net(output)\n",
    " deltaErrtot_ow = deltaErr_ow(deltaErr_out, deltaOut_net, hidden_layer)\n",
    " print(\"Delta Error with reference to output\", deltaErrtot_ow)\n",
    " return deltaErrtot_ow, deltaErr_out, deltaOut_net;\n",
    "\n",
    " \n",
    "# Delta Error with reference to input\n",
    "def delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net):\n",
    " deltaErrOut_hL = deltaOut_hL(deltaErr_out, deltaOut_net, network_bias[1])\n",
    " deltaErrOut_netHL = deltaOut_netHL(hidden_layer)\n",
    " deltaErrH_wi = deltaErr_wi(deltaErrOut_hL, deltaErrOut_netHL, network_bias[0])\n",
    " print(\"Delta Error with reference to input\", deltaErrH_wi)\n",
    " return deltaErrH_wi;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 2) (280,)\n",
      "(120, 2) (120,)\n"
     ]
    }
   ],
   "source": [
    " # Read in the the input file for the train/test datasets   \n",
    "inputDataFrame = pd.read_csv(\"moons400.csv\")  # instance variable unique to each instance\n",
    "y = inputDataFrame['Class'].values   \n",
    "\n",
    "#split up the dataset into training and test split 70/30 for training/test\n",
    "train_X, test_X, train_y, test_y = train_test_split(inputDataFrame, y, test_size=0.30)\n",
    "#print(train_X)\n",
    "#print(train_y)\n",
    "\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "(nsamples, nattribs) = np.shape(train_X)\n",
    "#subset_X = train_X[0:1]\n",
    "subset_X = train_X\n",
    "#input_y = train_y[0:1]\n",
    "input_y = train_y\n",
    "#print(\"Subset_X\", subset_X)\n",
    "#print(\"input_y\", input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs 280\n",
      "input Weights [[ 0.90094218  0.64800375]\n",
      " [ 0.63695274  0.75987986]]\n",
      "output Weights [[ 0.82291974  0.52800769]]\n",
      "Network Bias [[[ 0.95155327]]\n",
      "\n",
      " [[ 0.24708323]]]\n"
     ]
    }
   ],
   "source": [
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "# first line of moons.csv\n",
    "#input_X =[[2.07106946, 0.41152931]]\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = nattribs\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# rescale the inputs using normalization \n",
    "inputs = preprocessing.normalize(subset_X)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "input_weights = np.array(initialise_input_weights(n_inputs, n_hidden_inputs))\n",
    "output_weights = np.array(initialise_output_weights(n_hidden_inputs, n_outputs))\n",
    "#network_weights = initialise_weights(n_inputs, n_hidden_inputs, n_outputs)\n",
    "network_bias = np.array(initialise_bias(n_hidden_layer))\n",
    "\n",
    "print(\"Length of inputs\",len(inputs))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", input_weights)\n",
    "print(\"output Weights\", output_weights)\n",
    "print(\"Network Bias\", network_bias)\n",
    "\n",
    "#target = input_y\n",
    "#print(\"target\",len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "# Training section\n",
    "# use a loop to iterate through the dataset\n",
    "# and present records 1 by 1\n",
    "# for i in np.nditer(inputs, flags=['external_loop'], order='C'):\n",
    "#for i in range(len(inputs)):\n",
    "#    print (i)\n",
    "#    for t in range((target[i])):## this part is not working because it loops through the whole target array each time\n",
    "#        print(t)  ## not what we want\n",
    "\n",
    "# Training the model\n",
    "# returns adjusted weights\n",
    "print(len(input_y))\n",
    "def training_moon_set(inputs, input_y, input_weights, output_weights,network_bias):\n",
    " target = input_y\n",
    " threshold=1e-7\n",
    " #maxrounds=5000\n",
    " maxrounds=10\n",
    " iter = 0\n",
    "\n",
    " error = 99.0\n",
    " while abs(error) > threshold:\n",
    "    for i in range(len(inputs)):\n",
    "        # ----------START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "        hidden_layer = get_input_layer_activation(inputs[i], input_weights, network_bias[0])\n",
    "        output = forward_propagation(inputs, input_weights, output_weights, network_bias[0], network_bias[1])\n",
    "        print (\"output:\", output)\n",
    "\n",
    "        # call error function\n",
    "        error = calc_total_error(input_y[i], output )\n",
    "        # error = get_error(target[t], output)\n",
    "        print(\"Error Total:\", error)\n",
    "        # -------- END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    "         # ------- START OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "        (deltaErrtot_ow, deltaErr_out, deltaOut_net) = delta_error_wrt_output(output, hidden_layer, target[i])\n",
    "        deltaErrH_wi = delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net) \n",
    "        # calculate adjusted weights using function\n",
    "        output_weights = calc_adjusted_weights(output_weights, deltaErrtot_ow)\n",
    "        input_weights = calc_adjusted_weights(input_weights, deltaErrH_wi)\n",
    "        # --------- END OF BACK PROPAGATION OF HIDDEN LAYER\n",
    "\n",
    "    iter +=1\n",
    "    \n",
    "    if (iter > maxrounds):\n",
    "        break\n",
    "\n",
    "    print (\"\\nFinished after iteration \", iter, \" error =\", error, \"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "\n",
    "    print (\"Weights1 adjusted:\", input_weights)\n",
    "    print (\"Weights2 adjusted:\", output_weights)\n",
    "    \n",
    "    return input_weights,output_weights; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 2)\n",
      "(280,)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(inputs))\n",
    "print(np.shape(input_y))\n",
    "print(np.shape(input_weights))\n",
    "print(np.shape(output_weights))\n",
    "print(np.shape(network_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (280,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8ee09e386c03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# retrieve adjusted weights from the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_moon_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-4b0431df0fdf>\u001b[0m in \u001b[0;36mtraining_moon_set\u001b[1;34m(inputs, input_y, input_weights, output_weights, network_bias)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# ----------START OF FORWARD PROPAGATION FUNCTION CALLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"output:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-253ee90bf8fe>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(input, input_weights, output_weights, input_bias, output_bias)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m  \u001b[0msigmoid_input_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m  \u001b[0mactivation_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activation_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_input_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m  \u001b[1;31m#print(\"Forward Propagation Activation Output: \",activation_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-253ee90bf8fe>\u001b[0m in \u001b[0;36mget_input_layer_activation\u001b[1;34m(input, input_weights, input_bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m  \u001b[0minput_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m  \u001b[1;31m#print(\"input_layer_activation: \",input_layer_activation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m  \u001b[0mhidden_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_bias\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (280,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# retrieve adjusted weights from the training set\n",
    "input_weights,output_weights = training_moon_set(inputs, input_y, input_weights, output_weights, network_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs 280\n",
      "input Weights [[ 0.90094218  0.64800375]\n",
      " [ 0.63695274  0.75987986]]\n",
      "output Weights [[ 0.82291974  0.52800769]]\n",
      "Network Bias [[[ 0.95155327]]\n",
      "\n",
      " [[ 0.24708323]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (280,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-a1f90f51b346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# test get_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#output = get_output(hidden_layer, Weights2, bias_2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"output:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-253ee90bf8fe>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(input, input_weights, output_weights, input_bias, output_bias)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m  \u001b[0msigmoid_input_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m  \u001b[0mactivation_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activation_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_input_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m  \u001b[1;31m#print(\"Forward Propagation Activation Output: \",activation_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-253ee90bf8fe>\u001b[0m in \u001b[0;36mget_input_layer_activation\u001b[1;34m(input, input_weights, input_bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m  \u001b[0minput_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m  \u001b[1;31m#print(\"input_layer_activation: \",input_layer_activation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m  \u001b[0mhidden_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_bias\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (280,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# Same code as cell above - but with alot of the commented out code removed\n",
    "\n",
    "# use a loop to iterate through the dataset\n",
    "# and present records 1 by 1\n",
    "# for i in np.nditer(inputs, flags=['external_loop'], order='C'):\n",
    "#for i in range(len(inputs)):\n",
    "#    print (i)\n",
    "#    for t in range((target[i])):## this part is not working because it loops through the whole target array each time\n",
    "#        print(t)  ## not what we want\n",
    "#print(\"inputs\",inputs)\n",
    "print(\"Length of inputs\",len(inputs))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", input_weights)\n",
    "print(\"output Weights\", output_weights)\n",
    "print(\"Network Bias\", network_bias)\n",
    "target = input_y\n",
    "\n",
    "\n",
    "tol=1e-7\n",
    "#maxrounds=5000\n",
    "maxrounds=1\n",
    "iter = 0\n",
    "\n",
    "error = 99.0\n",
    "while abs(error) > tol:\n",
    "    for i in range(len(inputs)):\n",
    "    #print (i)\n",
    "    # START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "        hidden_layer = get_input_layer_activation(inputs[i], input_weights, network_bias[0])\n",
    "    #activation_output = get_activation_output(activation_input,output_weights, network_bias[1] )\n",
    "\n",
    "    # test get_sigmoid_output \n",
    "    #hidden_layer = get_sigmoid_output(Weights1, inputs[i], bias_1)\n",
    "    #print (\"hidden_layer result:\", hidden_layer)\n",
    "\n",
    "    # test get_output\n",
    "    #output = get_output(hidden_layer, Weights2, bias_2)\n",
    "        output = forward_propagation(inputs, input_weights, output_weights, network_bias[0], network_bias[1])\n",
    "        print (\"output:\", output)\n",
    "\n",
    "    # call error function\n",
    "        error = calc_total_error(input_y[i], output )\n",
    "     \n",
    "     #error = get_error(target[t], output)\n",
    "        print(\"Error Total:\", error)\n",
    "\n",
    " # END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    " # START OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    " #test function deltaErr_wrt_out\n",
    " #deltaErr_out = deltaErr_wrt_out(target[i], output)\n",
    " #deltaOut_net = deltaOut_wrt_net(output)\n",
    " \n",
    " #deltaErrtot_ow = deltaErr_ow(deltaErr_out, deltaOut_net, hidden_layer)\n",
    "        (deltaErrtot_ow, deltaErr_out, deltaOut_net) = delta_error_wrt_output(output, hidden_layer, target[i])\n",
    "  \n",
    " #END OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "    \n",
    " # START BACK PROPAGATION OF HIDDEN LAYER\n",
    " # calculate derivative of error at hidden layer\n",
    " # test function deltaOut_hL\n",
    " #deltaErrOut_hL = deltaOut_hL(deltaErr_out, deltaOut_net, network_bias[1])\n",
    " # print(deltaErrOut_hL)\n",
    " #deltaErrOut_netHL = deltaOut_netHL(hidden_layer)\n",
    " # print(deltaErrOut_netHL)\n",
    " \n",
    " # ************ this one needed for input weights\n",
    " #deltaErrH_wi = deltaErr_wi(deltaErrOut_hL, deltaErrOut_netHL, network_bias[0])\n",
    "        deltaErrH_wi = delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net) \n",
    "# print(deltaErrH_wi)\n",
    "\n",
    "    # same calculation as above but with Weights matrix \n",
    "    # calculate adjusted weights using function\n",
    "        output_weights = calc_adjusted_weights(output_weights, deltaErrtot_ow)\n",
    "        input_weights = calc_adjusted_weights(input_weights, deltaErrH_wi)\n",
    "\n",
    " # END OF BACK PROPAGATION OF HIDDEN LAYER\n",
    "\n",
    "    iter +=1\n",
    "    \n",
    "    if (iter > maxrounds):\n",
    "        break\n",
    "\n",
    "print (\"\\nFinished after iteration \", iter, \" error =\", error, \"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "\n",
    "print (\"Weights1 adjusted:\", input_weights)\n",
    "print (\"Weights2 adjusted:\", output_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://pypi.python.org/pypi/python-mnist\n",
    "# https://github.com/sorki/python-mnist/blob/master/mnist/loader.py\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "mndata = MNIST('./mnist')\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "# test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "processed_images = mndata.process_images_to_numpy(images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "target = np.array(labels)\n",
    "# print(target[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 6, 0, 6, 0, 6, 0, 6, 0]\n",
      "[0, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "filtered_labels = []\n",
    "filtered_images = []\n",
    "for i in range(len(target)):\n",
    "    if target[i]==0 or target[i]==6:\n",
    "        filtered_labels.append(target[i])\n",
    "        filtered_images.append(processed_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_labels)):\n",
    "    if filtered_labels[i]==6:\n",
    "        filtered_labels[i]=1\n",
    "\n",
    "# print(filtered_labels[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the lists to arrays\n",
    "# TRAIN MNIST DATA\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "filtered_images = np.array(filtered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: samples = 11841 , attribs = 784\n",
      "Length of inputs 11841\n",
      "input Weights 784\n",
      "output Weights 1\n",
      "Network Bias 2\n"
     ]
    }
   ],
   "source": [
    "# set up weights and biases for MNIST data, get nattribs value\n",
    "# TRAIN MNIST DATA\n",
    "\n",
    "(train_samples, train_shape) = np.shape(filtered_images)\n",
    "print (\"train: samples =\", train_samples, \", attribs =\", train_shape)\n",
    "\n",
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "# first line of moons.csv\n",
    "#input_X =[[2.07106946, 0.41152931]]\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = train_shape\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# normalise training data\n",
    "# rescale the inputs using normalization \n",
    "mnist_train = preprocessing.normalize(filtered_images)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "input_weights = np.array(initialise_input_weights(n_inputs, n_hidden_inputs))\n",
    "output_weights = np.array(initialise_output_weights(n_hidden_inputs, n_outputs))\n",
    "#network_weights = initialise_weights(n_inputs, n_hidden_inputs, n_outputs)\n",
    "network_bias = np.array(initialise_bias(n_hidden_layer))\n",
    "\n",
    "print(\"Length of inputs\",len(filtered_images))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", len(input_weights))\n",
    "print(\"output Weights\", len(output_weights))\n",
    "print(\"Network Bias\", len(network_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST TEST DATA\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "test_images = mndata.process_images_to_numpy(test_images)\n",
    "# print(processed_images[0:2])\n",
    "\n",
    "test_target = np.array(test_labels)\n",
    "\n",
    "# set up 2 lists to filter the MNIST data into\n",
    "# keeping only 0 and 6 to classify\n",
    "# TEST MNIST DATA\n",
    "\n",
    "filtered_test_labels = []\n",
    "filtered_test_images = []\n",
    "for i in range(len(test_target)):\n",
    "    if test_target[i]==0 or test_target[i]==6:\n",
    "        filtered_test_labels.append(test_target[i])\n",
    "        filtered_test_images.append(test_images[i])\n",
    "        \n",
    "# remap the value 6 to 1, so classification is binary\n",
    "\n",
    "# print(filtered_labels[0:10])\n",
    "\n",
    "for i in range(len(filtered_test_labels)):\n",
    "    if filtered_test_labels[i]==6:\n",
    "        filtered_test_labels[i]=1\n",
    "        \n",
    "# convert the lists to arrays\n",
    "# TEST MNIST DATA\n",
    "filtered_test_labels = np.array(filtered_test_labels)\n",
    "\n",
    "filtered_test_images = np.array(filtered_test_images)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_test_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
