{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training rate alpha\n",
    "alpha = 0.5\n",
    "\n",
    "# initialise the weights for the network based on the input layers, the number of hidden layers, the number of output layers\n",
    "def initialise_input_weights(n_inputs, n_hidden_inputs):\n",
    " hidden_layer_weights = list()\n",
    " for i in range(n_hidden_inputs):\n",
    "  for j in range(n_inputs):\n",
    "   weight = np.random.random(1)[0]\n",
    "   hidden_layer_weights.append(weight)\n",
    " \n",
    " input_weights = np.array([hidden_layer_weights])\n",
    " input_weights = np.reshape(input_weights, (n_inputs, n_hidden_inputs))\n",
    " return input_weights; \n",
    "\n",
    "def initialise_output_weights(n_hidden_inputs,n_outputs):\n",
    " output_layer_weights = list()\n",
    " for i in range(n_outputs):\n",
    "  for j in range(n_hidden_inputs):\n",
    "   weight = np.random.random(1)[0]\n",
    "   output_layer_weights.append(weight) \n",
    "  \n",
    " if n_outputs == 1:\n",
    "  output_weights = np.array([output_layer_weights])\n",
    " elif n_outputs > 1:\n",
    "  output_weights = np.array([output_layer_weights])  \n",
    "  output_weights = np.reshape(output_weights, (n_hidden_inputs,n_outputs))\n",
    " \n",
    " return output_weights;\n",
    "    \n",
    "#initialise the bias for the network based on the number of hidden layers and the output layer bias\n",
    "def initialise_bias(n_hidden_layer):\n",
    " hidden_layer_bias = list()    \n",
    " for i in range(n_hidden_layer):\n",
    "  bias = np.random.random(1)[0]\n",
    "  hidden_layer_bias.append(bias)\n",
    " \n",
    " output_layer_bias = [np.random.random(1)[0]]\n",
    " network_bias = [[hidden_layer_bias],[output_layer_bias]]\n",
    " return network_bias;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "# g(z) = 1/ 1 + e^-z\n",
    "def sigmoid(z):\n",
    " g = 1/(1 + np.exp(-z))\n",
    " return g;\n",
    "\n",
    "# --------------- FORWARD PROPAGATION  --------------------------\n",
    "# return the sigmoid_input_act - sigmoid function sum of the input layer activation \n",
    "# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\n",
    "def get_input_layer_activation(input, input_weights, input_bias):\n",
    " input_layer_activation = input * input_weights\n",
    " #print(\"input_layer_activation: \",input_layer_activation)\n",
    " hidden_layer_activation = np.sum(input_layer_activation, axis = 1) + input_bias * 1\n",
    " #print(\"Sum of Hidden Layer activation: \", hidden_layer_activation)\n",
    " sigmoid_input_activation = sigmoid(hidden_layer_activation)\n",
    " #print(\"Sigmoid function of Sum of Hidden Layer activation: \", sigmoid_input_activation)\n",
    " return sigmoid_input_activation;\n",
    "\n",
    "# get output layer activation for hidden layer\n",
    "# a1_3\n",
    "def get_activation_output(sigmoid_input_activation, output_weights, output_bias):\n",
    " output = (sigmoid_input_activation * output_weights)\n",
    " activation_output = sigmoid(np.sum(output) + output_bias * 1 )\n",
    " #print(\"Activation Output: \",activation_output)   \n",
    " return activation_output;\n",
    " \n",
    "def forward_propagation(input, input_weights, output_weights, input_bias, output_bias):\n",
    " sigmoid_input_activation = get_input_layer_activation(input, input_weights, input_bias)\n",
    " activation_output = get_activation_output(sigmoid_input_activation, output_weights, output_bias)\n",
    " #print(\"Forward Propagation Activation Output: \",activation_output)    \n",
    " return activation_output;   \n",
    "\n",
    "# Calculate the Total Error Sum of Squared Errors = âˆ‘ 1/2(Y-YP)^2 \n",
    "#E_total = 1/2(target_01 - out_01)^2 \n",
    "# sum of squared errors of prediction\n",
    "def calc_error(actual_y, target):\n",
    " error = 1/2 * np.power((actual_y - target), 2)\n",
    " #print(\"Error Total: \",error)\n",
    " return error;\n",
    "\n",
    "# If there is more than 1 node in the output layer, sum up the calc error\n",
    "def calc_total_error(actual_y, target):\n",
    " total_error = 0\n",
    " total_error += calc_error(actual_y, target)\n",
    " return total_error;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR BACK PROPAGATION OF OUTPUT\n",
    "# calculate derivative of error at output layer\n",
    "\n",
    "# Derivitive of error with reference to output\n",
    "# deriv_wrt_out = -(target - output)\n",
    "# change above to function\n",
    "def deltaErr_wrt_out(target, output):\n",
    " result = -(target - output)\n",
    " print(\"Derivitive of error with reference to output:\", result)\n",
    " return result;  \n",
    "\n",
    "# calculate the derivation of the error output wrt the net\n",
    "# derivout_wrt_net = output*(1-output)\n",
    "# change above to function\n",
    "def deltaOut_wrt_net(output):\n",
    " result = output * (1 - output)\n",
    " print(\"Derivitive of error output with reference to net output:\", result)\n",
    " return result;   \n",
    "\n",
    "# calculate derivative of error wrt to output layer weight OLW_Deriv\n",
    "# Output Layer Weight Derivitive\n",
    "def deltaErr_ow(deriv_wrt_out, derivout_wrt_net, activation):\n",
    " OLW_Deriv = deriv_wrt_out * derivout_wrt_net * activation\n",
    " print(\"Derivative of error with reference to output layer weight:\", OLW_Deriv)\n",
    " return OLW_Deriv;\n",
    "    \n",
    "# FUNCTIONS FOR BACK PROPAGATION OF HIDDEN LAYER\n",
    "# calculate derivative of error at hidden layer\n",
    "# deriv_out_wrt_hL =  Weights2 * deriv_wrt_out *derivout_wrt_net\n",
    "# print (deriv_out_wrt_hL)\n",
    "# convert above to function\n",
    "def deltaOut_hL(deriv_wrt_out, derivout_wrt_net, output_weights):\n",
    " deriv_out_wrt_hL = deriv_wrt_out * derivout_wrt_net * output_weights\n",
    " return deriv_out_wrt_hL;\n",
    "\n",
    "# derivitive output in relation to net of hidden layer activation\n",
    "# deriv_out_wrt_nethL = activation*(1-activation)\n",
    "# convert above to function\n",
    "def deltaOut_netHL(activation):\n",
    " activation = activation * (1 - activation)\n",
    " return activation;\n",
    "\n",
    "# Derivitive in relation to input_weights\n",
    "# deriv_wrt_wi = deriv_out_wrt_hL*deriv_out_wrt_nethL*Weights1\n",
    "# convert above to function\n",
    "def deltaErr_wi(deriv_out_wrt_hL, deriv_out_wrt_nethL, input_weights):\n",
    " deriv_wrt_wi = deriv_out_wrt_hL * deriv_out_wrt_nethL * input_weights\n",
    " return deriv_wrt_wi;\n",
    "\n",
    "# convert the above to a function\n",
    "def calc_adjusted_weights(W, deriv):\n",
    " W = W - (alpha * deriv)\n",
    " return W;\n",
    "\n",
    "# delta error with reference to output\n",
    "def delta_error_wrt_output(output, hidden_layer, target):\n",
    " deltaErr_out = deltaErr_wrt_out(target, output)\n",
    " deltaOut_net = deltaOut_wrt_net(output)\n",
    " deltaErrtot_ow = deltaErr_ow(deltaErr_out, deltaOut_net, hidden_layer)\n",
    " print(\"Delta Error with reference to output\", deltaErrtot_ow)\n",
    " return deltaErrtot_ow, deltaErr_out, deltaOut_net;\n",
    "\n",
    " \n",
    "# Delta Error with reference to input\n",
    "def delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net):\n",
    " deltaErrOut_hL = deltaOut_hL(deltaErr_out, deltaOut_net, network_bias[1])\n",
    " deltaErrOut_netHL = deltaOut_netHL(hidden_layer)\n",
    " deltaErrH_wi = deltaErr_wi(deltaErrOut_hL, deltaErrOut_netHL, network_bias[0])\n",
    " print(\"Delta Error with reference to input\", deltaErrH_wi)\n",
    " return deltaErrH_wi;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 2) (280,)\n",
      "(120, 2) (120,)\n"
     ]
    }
   ],
   "source": [
    " # Read in the the input file for the train/test datasets   \n",
    "inputDataFrame = pd.read_csv(\"moons400.csv\")  # instance variable unique to each instance\n",
    "y = inputDataFrame['Class'].values   \n",
    "\n",
    "#split up the dataset into training and test split 70/30 for training/test\n",
    "train_X, test_X, train_y, test_y = train_test_split(inputDataFrame, y, test_size=0.30)\n",
    "#print(train_X)\n",
    "#print(train_y)\n",
    "\n",
    "del train_X['Class']    # drop the 'Class' column from the Train and test dataframe\n",
    "del test_X['Class']\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "(nsamples, nattribs) = np.shape(train_X)\n",
    "#subset_X = train_X[0:1]\n",
    "subset_X = train_X\n",
    "#input_y = train_y[0:1]\n",
    "input_y = train_y\n",
    "#print(\"Subset_X\", subset_X)\n",
    "#print(\"input_y\", input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs 280\n",
      "input Weights [[ 0.83703539  0.46927968]\n",
      " [ 0.81644041  0.69611427]]\n",
      "output Weights [[ 0.2127937   0.30346592]]\n",
      "Network Bias [[[0.62740036152144996]], [[0.637633953610934]]]\n"
     ]
    }
   ],
   "source": [
    "# Test weights and bias initialisation based on network inputs, hidden layers, and outputs\n",
    "# first line of moons.csv\n",
    "#input_X =[[2.07106946, 0.41152931]]\n",
    "\n",
    "#(nsamples, nattribs) = np.shape(input_X)\n",
    "n_inputs = nattribs\n",
    "n_hidden_layer = 1\n",
    "n_hidden_inputs = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# rescale the inputs using normalization \n",
    "inputs = preprocessing.normalize(subset_X)\n",
    "#print(\"normalised inputs\",inputs)\n",
    "\n",
    "input_weights = initialise_input_weights(n_inputs, n_hidden_inputs)\n",
    "output_weights = initialise_output_weights(n_hidden_inputs, n_outputs)\n",
    "#network_weights = initialise_weights(n_inputs, n_hidden_inputs, n_outputs)\n",
    "network_bias = initialise_bias(n_hidden_layer)\n",
    "\n",
    "print(\"Length of inputs\",len(inputs))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", input_weights)\n",
    "print(\"output Weights\", output_weights)\n",
    "print(\"Network Bias\", network_bias)\n",
    "\n",
    "#target = input_y\n",
    "#print(\"target\",len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training section\n",
    "# use a loop to iterate through the dataset\n",
    "# and present records 1 by 1\n",
    "# for i in np.nditer(inputs, flags=['external_loop'], order='C'):\n",
    "#for i in range(len(inputs)):\n",
    "#    print (i)\n",
    "#    for t in range((target[i])):## this part is not working because it loops through the whole target array each time\n",
    "#        print(t)  ## not what we want\n",
    "\n",
    "# Training the model\n",
    "# returns adjusted weights\n",
    "def training_moon_set(inputs, input_y, input_weights, output_weights,network_bias):\n",
    " target = input_y\n",
    " tol=1e-7\n",
    " #maxrounds=5000\n",
    " maxrounds=1\n",
    " iter = 0\n",
    "\n",
    " error = 99.0\n",
    " while abs(error) > tol:\n",
    "    for i in range(len(inputs)):\n",
    "        # ----------START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "        hidden_layer = get_input_layer_activation(inputs[i], input_weights, network_bias[0])\n",
    "        output = forward_propagation(inputs, input_weights, output_weights, network_bias[0], network_bias[1])\n",
    "        print (\"output:\", output)\n",
    "\n",
    "        # call error function\n",
    "        error = calc_total_error(input_y[i], output )\n",
    "        # error = get_error(target[t], output)\n",
    "        print(\"Error Total:\", error)\n",
    "        # -------- END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    "         # ------- START OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "        (deltaErrtot_ow, deltaErr_out, deltaOut_net) = delta_error_wrt_output(output, hidden_layer, target[i])\n",
    "        deltaErrH_wi = delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net) \n",
    "        # calculate adjusted weights using function\n",
    "        output_weights = calc_adjusted_weights(output_weights, deltaErrtot_ow)\n",
    "        input_weights = calc_adjusted_weights(input_weights, deltaErrH_wi)\n",
    "        # --------- END OF BACK PROPAGATION OF HIDDEN LAYER\n",
    "\n",
    "    iter +=1\n",
    "    \n",
    "    if (iter > maxrounds):\n",
    "        break\n",
    "\n",
    "    print (\"\\nFinished after iteration \", iter, \" error =\", error, \"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "\n",
    "    print (\"Weights1 adjusted:\", input_weights)\n",
    "    print (\"Weights2 adjusted:\", output_weights)\n",
    "    \n",
    "    return input_weights,output_weights; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (280,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-8ee09e386c03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# retrieve adjusted weights from the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_moon_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-9299c0e245af>\u001b[0m in \u001b[0;36mtraining_moon_set\u001b[1;34m(inputs, input_y, input_weights, output_weights, network_bias)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# ----------START OF FORWARD PROPAGATION FUNCTION CALLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"output:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-253ee90bf8fe>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(input, input_weights, output_weights, input_bias, output_bias)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m  \u001b[0msigmoid_input_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m  \u001b[0mactivation_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activation_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_input_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m  \u001b[1;31m#print(\"Forward Propagation Activation Output: \",activation_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-253ee90bf8fe>\u001b[0m in \u001b[0;36mget_input_layer_activation\u001b[1;34m(input, input_weights, input_bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m  \u001b[0minput_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m  \u001b[1;31m#print(\"input_layer_activation: \",input_layer_activation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m  \u001b[0mhidden_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_bias\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (280,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# retrieve adjusted weights from the training set\n",
    "input_weights,output_weights = training_moon_set(inputs, input_y, input_weights, output_weights, network_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [[  9.97046070e-01   7.68058266e-02]\n",
      " [  9.99476541e-01  -3.23518657e-02]\n",
      " [  1.69124015e-01   9.85594779e-01]\n",
      " [  8.41335622e-01  -5.40513063e-01]\n",
      " [  9.31112595e-01   3.64731867e-01]\n",
      " [ -3.49231301e-01   9.37036551e-01]\n",
      " [  5.80853787e-01   8.14007910e-01]\n",
      " [  9.98130178e-01  -6.11240333e-02]\n",
      " [  8.86912162e-01   4.61938109e-01]\n",
      " [ -5.70366865e-01   8.21390065e-01]\n",
      " [  9.86399623e-01   1.64364791e-01]\n",
      " [  9.25032945e-01  -3.79886893e-01]\n",
      " [ -5.22229624e-01   8.52804913e-01]\n",
      " [  4.92653409e-01   8.70225614e-01]\n",
      " [ -1.53912707e-01   9.88084449e-01]\n",
      " [ -4.46708589e-01   8.94679516e-01]\n",
      " [  7.67618760e-01  -6.40906732e-01]\n",
      " [ -6.57926720e-01   7.53081955e-01]\n",
      " [  9.67421526e-01   2.53171070e-01]\n",
      " [  9.93963895e-01  -1.09707678e-01]\n",
      " [  9.67641817e-01  -2.52327792e-01]\n",
      " [ -6.88621265e-01   7.25121199e-01]\n",
      " [  9.99894548e-01   1.45221196e-02]\n",
      " [  9.39350263e-01   3.42959303e-01]\n",
      " [ -9.99534946e-01   3.04941418e-02]\n",
      " [  1.65250839e-02   9.99863451e-01]\n",
      " [  8.96503166e-01  -4.43037326e-01]\n",
      " [ -2.13405527e-01  -9.76963705e-01]\n",
      " [  9.91615239e-01   1.29225450e-01]\n",
      " [  3.77711487e-01   9.25923341e-01]\n",
      " [  8.67064470e-01  -4.98195950e-01]\n",
      " [  9.94936950e-01   1.00501077e-01]\n",
      " [  8.98741870e-01  -4.38478107e-01]\n",
      " [ -4.28011706e-01   9.03773190e-01]\n",
      " [  9.46618788e-01   3.22355192e-01]\n",
      " [  9.85105170e-01   1.71952911e-01]\n",
      " [  9.99222324e-01  -3.94302784e-02]\n",
      " [  8.26241894e-01   5.63315482e-01]\n",
      " [  9.73392454e-01  -2.29144346e-01]\n",
      " [  6.68179814e-01  -7.43999823e-01]\n",
      " [  9.34840113e-01   3.55068955e-01]\n",
      " [  8.03125957e-01  -5.95809280e-01]\n",
      " [ -6.02293193e-01   7.98274959e-01]\n",
      " [ -7.79700182e-01   6.26153037e-01]\n",
      " [  9.89236558e-01   1.46325089e-01]\n",
      " [  6.60997016e-01   7.50388530e-01]\n",
      " [  9.31455697e-01  -3.63854757e-01]\n",
      " [  9.03036021e-01  -4.29564831e-01]\n",
      " [  9.96521971e-01   8.33304312e-02]\n",
      " [  9.54337705e-01  -2.98729887e-01]\n",
      " [ -9.74045264e-01   2.26353317e-01]\n",
      " [ -3.50751227e-01   9.36468674e-01]\n",
      " [ -3.18399146e-01   9.47956741e-01]\n",
      " [  3.13913736e-01   9.49451508e-01]\n",
      " [  9.69747176e-01   2.44111479e-01]\n",
      " [  9.92191450e-01  -1.24724202e-01]\n",
      " [ -4.04316697e-01   9.14619051e-01]\n",
      " [  8.33692566e-01   5.52228853e-01]\n",
      " [  6.32670810e-01  -7.74420845e-01]\n",
      " [  9.84117880e-01  -1.77516192e-01]\n",
      " [  5.45657889e-02  -9.98510178e-01]\n",
      " [ -9.86858038e-01   1.61589641e-01]\n",
      " [ -7.17681902e-01   6.96371085e-01]\n",
      " [  7.27753990e-01   6.85838268e-01]\n",
      " [  9.82783056e-01  -1.84763267e-01]\n",
      " [  9.98599867e-01  -5.28990111e-02]\n",
      " [  9.80087299e-01  -1.98567084e-01]\n",
      " [  9.96827713e-01  -7.95896382e-02]\n",
      " [  9.56221265e-01   2.92644652e-01]\n",
      " [ -4.81238703e-01   8.76589591e-01]\n",
      " [  9.35987914e-01  -3.52032136e-01]\n",
      " [  3.56792471e-02   9.99363293e-01]\n",
      " [  9.95087219e-01   9.90021528e-02]\n",
      " [  6.62755015e-01  -7.48836290e-01]\n",
      " [  9.07573902e-01   4.19892381e-01]\n",
      " [  9.89531949e-01   1.44313970e-01]\n",
      " [  9.94639960e-01  -1.03398981e-01]\n",
      " [  4.86898893e-01   8.73458338e-01]\n",
      " [  4.29030001e-01   9.03290240e-01]\n",
      " [ -9.94456741e-01  -1.05146521e-01]\n",
      " [ -6.77885643e-01   7.35167366e-01]\n",
      " [ -5.40773178e-01  -8.41168455e-01]\n",
      " [  9.19154602e-01  -3.93896963e-01]\n",
      " [  9.88835201e-01   1.49013239e-01]\n",
      " [ -4.46265694e-01   8.94900514e-01]\n",
      " [  7.92655460e-01   6.09669846e-01]\n",
      " [ -8.28073125e-01   5.60620102e-01]\n",
      " [ -4.68664586e-01   8.83376197e-01]\n",
      " [  9.99693822e-01   2.47439348e-02]\n",
      " [  9.52018780e-01   3.06039609e-01]\n",
      " [  5.69159731e-03   9.99983803e-01]\n",
      " [  7.96356953e-01   6.04826920e-01]\n",
      " [  9.96452198e-01  -8.41606640e-02]\n",
      " [ -9.21396016e-01   3.88624988e-01]\n",
      " [  8.34813237e-01   5.50533250e-01]\n",
      " [  6.72452893e-01  -7.40139923e-01]\n",
      " [  9.98230264e-01  -5.94671409e-02]\n",
      " [ -9.92936377e-01   1.18648018e-01]\n",
      " [  6.11542205e-03   9.99981301e-01]\n",
      " [  9.75617380e-01  -2.19478307e-01]\n",
      " [  9.99001852e-01   4.46687719e-02]\n",
      " [  5.66073847e-01  -8.24354535e-01]\n",
      " [  9.82794726e-01   1.84701184e-01]\n",
      " [  8.53363021e-01  -5.21317134e-01]\n",
      " [  9.51216908e-01  -3.08522922e-01]\n",
      " [  6.64376374e-01   7.47398176e-01]\n",
      " [ -9.66515607e-01   2.56607838e-01]\n",
      " [  9.07631169e-01  -4.19768580e-01]\n",
      " [ -9.99480356e-01  -3.22338124e-02]\n",
      " [ -6.14834457e-02   9.98108103e-01]\n",
      " [  8.07333235e-01  -5.90095796e-01]\n",
      " [  6.82264513e-01   7.31105420e-01]\n",
      " [ -4.74993655e-01   8.79989220e-01]\n",
      " [  8.52455892e-01  -5.22799151e-01]\n",
      " [  8.34946059e-02   9.96508229e-01]\n",
      " [  9.84303391e-01   1.76484659e-01]\n",
      " [  9.85867091e-01  -1.67529338e-01]\n",
      " [  9.60528364e-01  -2.78182065e-01]\n",
      " [  7.07364729e-01   7.06848739e-01]\n",
      " [  3.31021670e-01   9.43623153e-01]\n",
      " [  9.83700122e-01  -1.79816768e-01]\n",
      " [ -9.11634707e-01   4.11001412e-01]\n",
      " [  2.88644320e-01   9.57436398e-01]\n",
      " [  9.74385379e-01   2.24884712e-01]\n",
      " [  9.83810485e-01   1.79211970e-01]\n",
      " [ -6.92773591e-01   7.21155151e-01]\n",
      " [  9.82879450e-01   1.84249794e-01]\n",
      " [  9.33771171e-01   3.57870648e-01]\n",
      " [  7.16869807e-01   6.97207056e-01]\n",
      " [  4.77571019e-01   8.78593149e-01]\n",
      " [  9.82629898e-01   1.85576088e-01]\n",
      " [ -9.58166321e-03   9.99954095e-01]\n",
      " [ -1.40499013e-01   9.90080819e-01]\n",
      " [ -7.73971485e-01   6.33220452e-01]\n",
      " [ -8.98199616e-01   4.39587817e-01]\n",
      " [  9.84976088e-01   1.72690783e-01]\n",
      " [  8.34778049e-01  -5.50586605e-01]\n",
      " [ -5.37268781e-01   8.43411084e-01]\n",
      " [  9.75802451e-01   2.18654013e-01]\n",
      " [ -9.23662360e-01   3.83207312e-01]\n",
      " [ -9.98385554e-01   5.68004099e-02]\n",
      " [  9.49782224e-01  -3.12911693e-01]\n",
      " [  9.95267244e-01  -9.71756773e-02]\n",
      " [  9.42700982e-01  -3.33638815e-01]\n",
      " [ -1.64281053e-01   9.86413572e-01]\n",
      " [  6.76919031e-01   7.36057488e-01]\n",
      " [ -2.05217052e-01   9.78716487e-01]\n",
      " [  9.99292473e-01  -3.76105624e-02]\n",
      " [  9.35196716e-01  -3.54128653e-01]\n",
      " [ -9.83476670e-01   1.81034913e-01]\n",
      " [  9.88805575e-01  -1.49209701e-01]\n",
      " [  4.81200947e-01   8.76610318e-01]\n",
      " [ -3.42587570e-01   9.39485900e-01]\n",
      " [  9.96593630e-01  -8.24690018e-02]\n",
      " [ -1.72225273e-01   9.85057590e-01]\n",
      " [ -4.74507336e-01   8.80251548e-01]\n",
      " [  8.13211145e-01   5.81968756e-01]\n",
      " [ -9.94865864e-01   1.01202336e-01]\n",
      " [  9.99095120e-01  -4.25316577e-02]\n",
      " [  9.87139925e-01  -1.59858591e-01]\n",
      " [  9.92617477e-01   1.21287031e-01]\n",
      " [  7.93222960e-01  -6.08931307e-01]\n",
      " [  9.48717754e-01  -3.16124378e-01]\n",
      " [  5.56470392e-01   8.30867440e-01]\n",
      " [  4.60089479e-01   8.87872553e-01]\n",
      " [ -7.06118680e-01   7.08093504e-01]\n",
      " [  6.86967970e-01   7.26687696e-01]\n",
      " [ -9.95454101e-01   9.52424986e-02]\n",
      " [  8.89699269e-01   4.56547052e-01]\n",
      " [  7.93329198e-01  -6.08792890e-01]\n",
      " [  7.26986398e-01  -6.86651860e-01]\n",
      " [  9.02266693e-01  -4.31178402e-01]\n",
      " [  9.99677833e-01   2.53816843e-02]\n",
      " [  6.32441145e-01   7.74608416e-01]\n",
      " [  1.35308959e-01   9.90803455e-01]\n",
      " [  6.51367837e-01  -7.58762111e-01]\n",
      " [ -5.78072420e-01   8.15985464e-01]\n",
      " [  9.84777038e-01  -1.73822282e-01]\n",
      " [ -3.04550735e-01   9.52496115e-01]\n",
      " [  3.38670812e-01   9.40904927e-01]\n",
      " [  9.98601906e-01   5.28605069e-02]\n",
      " [  9.94230649e-01  -1.07263306e-01]\n",
      " [ -3.12144311e-01   9.50034699e-01]\n",
      " [  7.52824764e-01   6.58220992e-01]\n",
      " [ -3.09012137e-01   9.51058094e-01]\n",
      " [ -9.91160900e-01   1.32665254e-01]\n",
      " [ -3.30666872e-01   9.43747540e-01]\n",
      " [  9.98924279e-01  -4.63711720e-02]\n",
      " [  3.07878576e-01   9.51425658e-01]\n",
      " [ -4.04226166e-01   9.14659066e-01]\n",
      " [  5.70806835e-01  -8.21084379e-01]\n",
      " [  9.84585090e-01   1.74906260e-01]\n",
      " [  8.38861084e-01   5.44345553e-01]\n",
      " [  9.21479355e-01   3.88427339e-01]\n",
      " [  9.56078512e-01  -2.93110693e-01]\n",
      " [ -7.88616838e-01   6.14884934e-01]\n",
      " [  2.70617590e-02   9.99633764e-01]\n",
      " [ -3.19638153e-01   9.47539683e-01]\n",
      " [ -8.28162982e-01   5.60487355e-01]\n",
      " [  9.99999613e-01   8.80270368e-04]\n",
      " [  9.75486101e-01   2.20061052e-01]\n",
      " [  3.72943777e-01   9.27853943e-01]\n",
      " [  7.66452569e-02   9.97058426e-01]\n",
      " [  9.95474162e-01  -9.50325901e-02]\n",
      " [  7.28595302e-01  -6.84944440e-01]\n",
      " [  8.34509062e-01  -5.50994215e-01]\n",
      " [  9.28486967e-01   3.71364984e-01]\n",
      " [  9.74476478e-01  -2.24489629e-01]\n",
      " [  1.94699465e-01   9.80862946e-01]\n",
      " [ -4.09594686e-01   9.12267610e-01]\n",
      " [  9.71486903e-01  -2.37093224e-01]\n",
      " [  9.99360251e-01   3.57643380e-02]\n",
      " [  9.25530025e-01  -3.78674230e-01]\n",
      " [ -9.66588142e-01   2.56334477e-01]\n",
      " [ -8.99136956e-01   4.37667379e-01]\n",
      " [  9.99880103e-01   1.54847904e-02]\n",
      " [  7.46674456e-01  -6.65189640e-01]\n",
      " [ -8.15782782e-01   5.78358412e-01]\n",
      " [  8.17707605e-01  -5.75633801e-01]\n",
      " [  9.12882256e-01  -4.08222963e-01]\n",
      " [ -2.48975527e-02   9.99690008e-01]\n",
      " [  9.94316901e-01  -1.06460796e-01]\n",
      " [  3.30712540e-01   9.43731538e-01]\n",
      " [  9.39112993e-01  -3.43608479e-01]\n",
      " [  7.86566102e-01  -6.17506087e-01]\n",
      " [  1.41631539e-01   9.89919445e-01]\n",
      " [  8.80750600e-01  -4.73580384e-01]\n",
      " [  9.76511823e-01   2.15463825e-01]\n",
      " [ -7.29629754e-01   6.83842395e-01]\n",
      " [  9.78835325e-01   2.04649473e-01]\n",
      " [ -4.22532714e-01   9.06347674e-01]\n",
      " [  9.90762374e-01  -1.35609433e-01]\n",
      " [  8.87095831e-01  -4.61585298e-01]\n",
      " [  5.64772226e-01   8.25246831e-01]\n",
      " [  3.54120821e-01   9.35199682e-01]\n",
      " [ -1.34879194e-01   9.90862050e-01]\n",
      " [  9.57444836e-01  -2.88616330e-01]\n",
      " [ -1.66778828e-01   9.85994332e-01]\n",
      " [ -5.19166922e-01   8.54672865e-01]\n",
      " [  3.72980708e-01   9.27839098e-01]\n",
      " [  8.98206193e-01   4.39574379e-01]\n",
      " [ -1.31067793e-01   9.91373408e-01]\n",
      " [  9.95539672e-01   9.43438519e-02]\n",
      " [  4.82268473e-01   8.76023470e-01]\n",
      " [  4.18975744e-01   9.07997426e-01]\n",
      " [  5.28847350e-01   8.48716961e-01]\n",
      " [  9.86544371e-01   1.63493744e-01]\n",
      " [  8.24936768e-01   5.65225025e-01]\n",
      " [  2.31585809e-02   9.99731804e-01]\n",
      " [  8.72252479e-01  -4.89055838e-01]\n",
      " [  9.58247066e-01   2.85941533e-01]\n",
      " [ -9.23680071e-01   3.83164620e-01]\n",
      " [  9.99999445e-01  -1.05387827e-03]\n",
      " [  8.94590264e-01  -4.46887302e-01]\n",
      " [  9.97846254e-01   6.55961383e-02]\n",
      " [  9.84449545e-01   1.75667562e-01]\n",
      " [ -8.68092163e-01   4.96403058e-01]\n",
      " [ -9.08911617e-01   4.16988816e-01]\n",
      " [ -4.96092237e-01   8.68269827e-01]\n",
      " [  9.13725967e-01  -4.06330971e-01]\n",
      " [  8.41052172e-01   5.40954012e-01]\n",
      " [  9.62013112e-01  -2.73003246e-01]\n",
      " [  8.17125162e-01  -5.76460293e-01]\n",
      " [ -1.74340035e-01   9.84685509e-01]\n",
      " [  4.47496770e-01   8.94285548e-01]\n",
      " [  9.96874616e-01  -7.89999950e-02]\n",
      " [ -9.20357643e-01   3.91077754e-01]\n",
      " [ -9.85058068e-01   1.72222540e-01]\n",
      " [  9.75896613e-01   2.18233361e-01]\n",
      " [  9.99982580e-01  -5.90252578e-03]\n",
      " [  9.99375030e-01   3.53489798e-02]\n",
      " [ -9.87181413e-01   1.59602188e-01]\n",
      " [  9.16097500e-01  -4.00955571e-01]\n",
      " [  1.43312397e-01   9.89677501e-01]\n",
      " [ -9.68431067e-01   2.49281504e-01]\n",
      " [  3.39078628e-01  -9.40758037e-01]\n",
      " [ -7.91891176e-01   6.10662236e-01]\n",
      " [  8.42660839e-01   5.38444715e-01]\n",
      " [  9.62062150e-01   2.72830387e-01]\n",
      " [  9.35330586e-01  -3.53774921e-01]]\n",
      "Length of inputs 280\n",
      "actual y [0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
      " 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
      " 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
      " 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1]\n",
      "input Weights [[ 0.83703539  0.46927968]\n",
      " [ 0.81644041  0.69611427]]\n",
      "output Weights [[ 0.2127937   0.30346592]]\n",
      "Network Bias [[[0.62740036152144996]], [[0.637633953610934]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (280,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-44e233e5a9a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# test get_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#output = get_output(hidden_layer, Weights2, bias_2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"output:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-253ee90bf8fe>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(input, input_weights, output_weights, input_bias, output_bias)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m  \u001b[0msigmoid_input_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m  \u001b[0mactivation_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activation_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_input_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m  \u001b[1;31m#print(\"Forward Propagation Activation Output: \",activation_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-253ee90bf8fe>\u001b[0m in \u001b[0;36mget_input_layer_activation\u001b[1;34m(input, input_weights, input_bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# a_n^2 = f(W_n1^1 x1 + W_n2^1 x2 + W_n3^1 x3 +b_n^1 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_input_layer_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m  \u001b[0minput_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m  \u001b[1;31m#print(\"input_layer_activation: \",input_layer_activation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m  \u001b[0mhidden_layer_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_bias\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (280,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# Same code as cell above - but with alot of the commented out code removed\n",
    "\n",
    "# use a loop to iterate through the dataset\n",
    "# and present records 1 by 1\n",
    "# for i in np.nditer(inputs, flags=['external_loop'], order='C'):\n",
    "#for i in range(len(inputs)):\n",
    "#    print (i)\n",
    "#    for t in range((target[i])):## this part is not working because it loops through the whole target array each time\n",
    "#        print(t)  ## not what we want\n",
    "#print(\"inputs\",inputs)\n",
    "print(\"Length of inputs\",len(inputs))\n",
    "#print(\"actual y\",input_y)\n",
    "print(\"input Weights\", input_weights)\n",
    "print(\"output Weights\", output_weights)\n",
    "print(\"Network Bias\", network_bias)\n",
    "target = input_y\n",
    "\n",
    "\n",
    "tol=1e-7\n",
    "#maxrounds=5000\n",
    "maxrounds=1\n",
    "iter = 0\n",
    "\n",
    "error = 99.0\n",
    "while abs(error) > tol:\n",
    "    for i in range(len(inputs)):\n",
    "    #print (i)\n",
    "    # START OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "        hidden_layer = get_input_layer_activation(inputs[i], input_weights, network_bias[0])\n",
    "    #activation_output = get_activation_output(activation_input,output_weights, network_bias[1] )\n",
    "\n",
    "    # test get_sigmoid_output \n",
    "    #hidden_layer = get_sigmoid_output(Weights1, inputs[i], bias_1)\n",
    "    #print (\"hidden_layer result:\", hidden_layer)\n",
    "\n",
    "    # test get_output\n",
    "    #output = get_output(hidden_layer, Weights2, bias_2)\n",
    "        output = forward_propagation(inputs, input_weights, output_weights, network_bias[0], network_bias[1])\n",
    "        print (\"output:\", output)\n",
    "\n",
    "    # call error function\n",
    "        error = calc_total_error(input_y[i], output )\n",
    "     \n",
    "     #error = get_error(target[t], output)\n",
    "        print(\"Error Total:\", error)\n",
    "\n",
    " # END OF FORWARD PROPAGATION FUNCTION CALLS\n",
    "\n",
    " # START OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    " #test function deltaErr_wrt_out\n",
    " #deltaErr_out = deltaErr_wrt_out(target[i], output)\n",
    " #deltaOut_net = deltaOut_wrt_net(output)\n",
    " \n",
    " #deltaErrtot_ow = deltaErr_ow(deltaErr_out, deltaOut_net, hidden_layer)\n",
    "        (deltaErrtot_ow, deltaErr_out, deltaOut_net) = delta_error_wrt_output(output, hidden_layer, target[i])\n",
    "  \n",
    " #END OF BACK PROPAGATION FOR OUTPUT LAYER\n",
    "    \n",
    " # START BACK PROPAGATION OF HIDDEN LAYER\n",
    " # calculate derivative of error at hidden layer\n",
    " # test function deltaOut_hL\n",
    " #deltaErrOut_hL = deltaOut_hL(deltaErr_out, deltaOut_net, network_bias[1])\n",
    " # print(deltaErrOut_hL)\n",
    " #deltaErrOut_netHL = deltaOut_netHL(hidden_layer)\n",
    " # print(deltaErrOut_netHL)\n",
    " \n",
    " # ************ this one needed for input weights\n",
    " #deltaErrH_wi = deltaErr_wi(deltaErrOut_hL, deltaErrOut_netHL, network_bias[0])\n",
    "        deltaErrH_wi = delta_error_wrt_input(hidden_layer, network_bias, deltaErr_out, deltaOut_net) \n",
    "# print(deltaErrH_wi)\n",
    "\n",
    "    # same calculation as above but with Weights matrix \n",
    "    # calculate adjusted weights using function\n",
    "        output_weights = calc_adjusted_weights(output_weights, deltaErrtot_ow)\n",
    "        input_weights = calc_adjusted_weights(input_weights, deltaErrH_wi)\n",
    "\n",
    " # END OF BACK PROPAGATION OF HIDDEN LAYER\n",
    "\n",
    "    iter +=1\n",
    "    \n",
    "    if (iter > maxrounds):\n",
    "        break\n",
    "\n",
    "print (\"\\nFinished after iteration \", iter, \" error =\", error, \"target=\", target[i], \", output=\", output, \"input final=\", inputs[i])    \n",
    "\n",
    "print (\"Weights1 adjusted:\", input_weights)\n",
    "print (\"Weights2 adjusted:\", output_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
